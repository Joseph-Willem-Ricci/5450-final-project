{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joseph-Willem-Ricci/5450-final-project/blob/master/CIS5450_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5RzHfJIP-OW"
      },
      "source": [
        "# CIT 5450 Final Project\n",
        "Joanne Crean, Juan Goleniowski, Joseph Ricci"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction/Background"
      ],
      "metadata": {
        "id": "e0aECJXRIuTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hi there! Welcome to Joe, Juan, and Joanne's CIS 545 final project from Spring 2023. \n",
        "\n",
        "We are curious about what makes a song a “hit”. Is it a certain quality of the music that makes a song popular? Or how famous an artist already is? Do lyrical themes play a role? Is virality on social media a major driver of success? \n",
        "\n",
        "We found Kaggle projects where teams have studied trends in the Spotify charts but we haven’t identified a study that goes further than audio features to look at the artist’s profile and the lyrical themes. \n",
        "\n",
        "We will apply some models to see if we can predict how popular a song is depending on various features, and along the way we will try to gain some insight into different trends, song similarity, and which features contribute the most to a song's success. We will take the song’s audio features, the artist’s previous success in the Spotify charts, TikTok popularity and the lyrics into account. \n",
        "\n",
        "The datasets that we are using are:\n",
        "*  Spotify song rankings 2017-2021: https://www.kaggle.com/datasets/dhruvildave/spotify-charts\n",
        "*  Spotify song audio features:\n",
        "  - https://www.kaggle.com/datasets/rodolfofigueroa/spotify-12m-songs\n",
        "  - https://www.kaggle.com/rodolfofigueroa/spotify-12m-songs\n",
        "  - https://www.kaggle.com/muhmores/spotify-top-100-songs-of-20152019\n",
        "  - https://www.kaggle.com/sashankpillai/spotify-top-200-charts-20202021\n",
        "  - https://www.kaggle.com/maharshipandya/-spotify-tracks-dataset\n",
        "  - https://www.kaggle.com/vatsalmavani/spotify-dataset\n",
        "  - https://www.kaggle.com/nandhakumarss/spotify-song-tracks\n",
        "  - https://www.kaggle.com/elemento/music-albums-popularity-prediction\n",
        "*  The most popular songs on Tiktok in 2019-2021\n",
        "https://www.kaggle.com/datasets?search=tiktok+popular+songs\n",
        "*  Lyrics - dataset we made ourselves from https://genius.com/, available at https://www.kaggle.com/datasets/joannecrean/spotify-songs-lyrics\n",
        "*  Artist profile - features engineered from the Spotify dataset looking at the artist's success in the previous year.\n",
        "\n"
      ],
      "metadata": {
        "id": "TxD1P7pPGZ6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "sJ-O7tGYIqwQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMHK47vsTN2T"
      },
      "source": [
        "## Libraries and data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTHg3UfjQbIp"
      },
      "outputs": [],
      "source": [
        "# Installs\n",
        "%%capture\n",
        "!pip install -q kaggle\n",
        "!pip install pandasql\n",
        "!pip install sqlalchemy==1.4.46\n",
        "!pip install translate\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwAn5PY1_KU6"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import pandasql as ps\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import euclidean_distances\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "import gc\n",
        "from sklearn.impute import KNNImputer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import re\n",
        "from translate import Translator\n",
        "from pandas.plotting import scatter_matrix\n",
        "from langdetect import detect\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q53w-KnA7wi"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "drive.mount('/content/drive')\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxzLvzzIB-fi"
      },
      "outputs": [],
      "source": [
        "# Download Datasets\n",
        "%%capture\n",
        "# Spotify Top 200 and Viral 50 Charts for 2017 through 2021 Dataset\n",
        "!!kaggle datasets download -d dhruvildave/spotify-charts\n",
        "\n",
        "# Spotify Audio Features Datasets\n",
        "!!kaggle datasets download -d rodolfofigueroa/spotify-12m-songs\n",
        "!!kaggle datasets download -d muhmores/spotify-top-100-songs-of-20152019\n",
        "!!kaggle datasets download -d sashankpillai/spotify-top-200-charts-20202021\n",
        "!!kaggle datasets download -d maharshipandya/-spotify-tracks-dataset\n",
        "!!kaggle datasets download -d vatsalmavani/spotify-dataset\n",
        "!!kaggle datasets download -d nandhakumarss/spotify-song-tracks\n",
        "!!kaggle datasets download -d elemento/music-albums-popularity-prediction\n",
        "\n",
        "# TikTok Popular Songs Dataset\n",
        "!!kaggle datasets download -d sveta151/tiktok-popular-songs-2019\n",
        "!!kaggle datasets download -d sveta151/tiktok-popular-songs-2020\n",
        "!!kaggle datasets download -d sveta151/tiktok-popular-songs-2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WPqLyOGCp8B"
      },
      "outputs": [],
      "source": [
        "# Unzip Datasets\n",
        "%%capture\n",
        "!unzip /content/spotify-charts.zip\n",
        "!unzip /content/spotify-12m-songs.zip\n",
        "!unzip /content/spotify-top-100-songs-of-20152019.zip\n",
        "!unzip /content/spotify-top-200-charts-20202021.zip\n",
        "!unzip /content/-spotify-tracks-dataset.zip\n",
        "!unzip /content/spotify-dataset.zip\n",
        "!unzip /content/spotify-song-tracks.zip\n",
        "!unzip /content/music-albums-popularity-prediction.zip\n",
        "!unzip /content/tiktok-popular-songs-2019\n",
        "!unzip /content/tiktok-popular-songs-2020\n",
        "!unzip /content/tiktok-popular-songs-2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AlyHOELYRnS"
      },
      "outputs": [],
      "source": [
        "# Clean up directory to save space\n",
        "%%capture\n",
        "!rm sample_data/*\n",
        "!rm -d sample_data\n",
        "!rm ./*.zip\n",
        "!rm sample_solution.csv\n",
        "!rm ./*.xlsx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEbMv8CEQMJG"
      },
      "outputs": [],
      "source": [
        "# Read the csv files and save them to pandas dataframes\n",
        "df_charts = pd.read_csv('charts.csv')\n",
        "df_song_features_1 = pd.read_csv('tracks_features.csv')\n",
        "df_song_features_2 = pd.read_csv('Spotify 2010 - 2019 Top 100.csv')\n",
        "df_song_features_3 = pd.read_csv('spotify_dataset.csv')\n",
        "df_song_features_4 = pd.read_csv('dataset.csv')\n",
        "df_song_features_5 = pd.read_csv('data/data.csv')\n",
        "df_song_features_6 = pd.read_csv('SpotifyFeatures.csv')\n",
        "df_song_features_7 = pd.read_csv('train.csv')  # contains three songs per row, expands to 8 and 9 below\n",
        "df_song_features_10 = pd.read_csv('test.csv')  # contains three songs per row, expands to 11 and 12 below\n",
        "df_tiktok_19 = pd.read_csv('TikTok_songs_2019.csv')\n",
        "df_tiktok_20 = pd.read_csv('TikTok_songs_2020.csv')\n",
        "df_tiktok_21 = pd.read_csv('TikTok_songs_2021.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsiSHzoWYAfn"
      },
      "outputs": [],
      "source": [
        "# Clean up remaining files\n",
        "!rm data/*\n",
        "!rm -d data\n",
        "!rm ./*.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data wrangling and EDA"
      ],
      "metadata": {
        "id": "1kcXjhV8Ik_z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hmg7zVxaI-6"
      },
      "source": [
        "## Spotify Top 200 and Viral 50 Charts EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a quick look at the charts dataset to see what we are dealing with."
      ],
      "metadata": {
        "id": "jbQuK8J6WSy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_charts.head()"
      ],
      "metadata": {
        "id": "BG7SNHsxWZOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row in `df_charts` is uniquely identified by a song title, artist, date (2017 through 2021), region. This entry then also contains chart rank for that region, url, type of chart (viral50 or top200), and stream count for that day in that country. This is to say that one song may appear up to 365 times per country. We will eventually be grouping this data set by year, title and artist, so that we have the maximum ranking for a given song in a given year in all countries, but first, let's do some basic clean up so that the aggregation methods can run."
      ],
      "metadata": {
        "id": "o6gbbrRoX6ML"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv90Vf_y_d_M"
      },
      "outputs": [],
      "source": [
        "# Create a separate column for Year\n",
        "df_charts['Year'] = df_charts['date'].str.extract(r'(\\d{4})', expand=False)\n",
        "\n",
        "# Cast 'Year' as Int\n",
        "df_charts['Year'] = df_charts['Year'].astype(int)\n",
        "\n",
        "#remove ID, trend and streams columns\n",
        "df_charts.drop(['url', 'trend', 'streams'], axis=1, inplace=True)\n",
        "\n",
        "#Remove NaN from dataset\n",
        "df_charts.dropna(inplace=True)\n",
        "\n",
        "# Show datatypes to confirm we haven't missed anything\n",
        "df_charts.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfi4tbmeaYPc"
      },
      "source": [
        "## Spotify Audio Feature Dataset Wrangling and EDA\n",
        "\n",
        "Since the Spotify charts dataset does not come with audio features the songs, there is no guarantee that we can find datasets that include audio features for all of the songs in the charts. One of the first questions to address then is\n",
        "\n",
        "1. Can we find audio feature data for a suitably large fraction of the songs in the Charts dataset?\n",
        "\n",
        "Additionally, since we'll be combining multiple audio features datasets, there is no guarantee that those datasets share the same features, so\n",
        "\n",
        "2. What fields do the datasets share?\n",
        "\n",
        "There are many datasets available on kaggle that contain Spotify audio features, but not all are very usable, clean, large enough, for the same years, or containing a large enough set of features. So we started by searching on kaggle for as many data sets as we could find that seemed big, had high usability, many features, and that were at least not explicitly for songs released outside of 2017 - 2021.\n",
        "\n",
        "We eventually settled on the following set of datasets for audio features, at which point we began wrangling, cleaning, and understanding how many songs in `charts_df` we could find matching features for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLjoplKkLYD2"
      },
      "outputs": [],
      "source": [
        "# First, since we want to find song features for as many of the songs in 'df_charts' as possible, let's start by\n",
        "# projecting df_charts by song title and artist name, which will uniquely identify each song, and then dropping duplicates.\n",
        "# This will make it easier to do big joins with the songs in charts_df to see if we have matches\n",
        "df_charts_songs_artists = df_charts[['title', 'artist']].drop_duplicates()\n",
        "\n",
        "# We will omit the section where we visually inspected each of the 8 features data sets, but through this process we\n",
        "# discovered that some columns were formatted differently in different datasets, i.e. if there were mutliple artists,\n",
        "# some datasets listed them like \"Portico Quartet, Hania Rani\", and some listed them like ['Portico Quartet', 'Hania Rani']\n",
        "# so below we clean extranneous characters from artist columns if necessary to arrive at a consistent format\n",
        "df_song_features_1['artists'] = df_song_features_1['artists'].str.replace('[', '').str.replace(']', '').str.replace(\"'\", '')\n",
        "df_song_features_5['artists'] = df_song_features_5['artists'].str.replace('[', '').str.replace(']', '').str.replace(\"'\", '')\n",
        "df_song_features_7['artists'] = df_song_features_7['artists'].str.replace(', ', '')\n",
        "df_song_features_10['artists'] = df_song_features_10['artists'].str.replace(', ', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es9cOCyBb6IW"
      },
      "outputs": [],
      "source": [
        "# Rename song title \"title\" and \"artist\" columns in each dataframe and \n",
        "# create separate dataframes for features_7 and features_10, which each have three columns for each field for three different songs\n",
        "df_song_features_1 = df_song_features_1.rename(columns={'name': 'title', 'artists': 'artist'})\n",
        "# _song_features_2 already has fields 'title' and 'artist'\n",
        "df_song_features_3 = df_song_features_3.rename(columns={'Song Name': 'title', 'Artist': 'artist'})\n",
        "df_song_features_4 = df_song_features_4.rename(columns={'track_name': 'title', 'artists': 'artist'})\n",
        "df_song_features_5 = df_song_features_5.rename(columns={'name': 'title', 'artists': 'artist'})\n",
        "df_song_features_6 = df_song_features_6.rename(columns={'track_name': 'title', 'artist_name': 'artist'})\n",
        "df_song_features_8 = df_song_features_7.rename(columns={'t_name1': 'title', 'artists': 'artist'}).drop(columns=['t_name0', 't_name2', 't_dur0', 't_dur2', 't_dance0', 't_dance2', 't_energy0', 't_energy2', 't_key0', 't_key2', 't_mode0', 't_mode2', 't_speech0', 't_speech2', 't_acous0', 't_acous2', 't_ins0', 't_ins2', 't_live0', 't_live2', 't_val0', 't_val2', 't_tempo0', 't_tempo2', 't_sig0', 't_sig2'])\n",
        "df_song_features_9 = df_song_features_7.rename(columns={'t_name2': 'title', 'artists': 'artist'}).drop(columns=['t_name0', 't_name1', 't_dur0', 't_dur1', 't_dance0', 't_dance1', 't_energy0', 't_energy1', 't_key0', 't_key1', 't_mode0', 't_mode1', 't_speech0', 't_speech1', 't_acous0', 't_acous1', 't_ins0', 't_ins1', 't_live0', 't_live1', 't_val0', 't_val1', 't_tempo0', 't_tempo1', 't_sig0', 't_sig1'])\n",
        "df_song_features_7 = df_song_features_7.rename(columns={'t_name0': 'title', 'artists': 'artist'}).drop(columns=['t_name1', 't_name2', 't_dur1', 't_dur2', 't_dance1', 't_dance2', 't_energy1', 't_energy2', 't_key1', 't_key2', 't_mode1', 't_mode2', 't_speech1', 't_speech2', 't_acous1', 't_acous2', 't_ins1', 't_ins2', 't_live1', 't_live2', 't_val1', 't_val2', 't_tempo1', 't_tempo2', 't_sig1', 't_sig2'])\n",
        "df_song_features_11 = df_song_features_10.rename(columns={'t_name1': 'title', 'artists': 'artist'}).drop(columns=['t_name0', 't_name2', 't_dur0', 't_dur2', 't_dance0', 't_dance2', 't_energy0', 't_energy2', 't_key0', 't_key2', 't_mode0', 't_mode2', 't_speech0', 't_speech2', 't_acous0', 't_acous2', 't_ins0', 't_ins2', 't_live0', 't_live2', 't_val0', 't_val2', 't_tempo0', 't_tempo2', 't_sig0', 't_sig2'])\n",
        "df_song_features_12 = df_song_features_10.rename(columns={'t_name2': 'title', 'artists': 'artist'}).drop(columns=['t_name0', 't_name1', 't_dur0', 't_dur1', 't_dance0', 't_dance1', 't_energy0', 't_energy1', 't_key0', 't_key1', 't_mode0', 't_mode1', 't_speech0', 't_speech1', 't_acous0', 't_acous1', 't_ins0', 't_ins1', 't_live0', 't_live1', 't_val0', 't_val1', 't_tempo0', 't_tempo1', 't_sig0', 't_sig1'])\n",
        "df_song_features_10 = df_song_features_10.rename(columns={'t_name0': 'title', 'artists': 'artist'}).drop(columns=['t_name1', 't_name2', 't_dur1', 't_dur2', 't_dance1', 't_dance2', 't_energy1', 't_energy2', 't_key1', 't_key2', 't_mode1', 't_mode2', 't_speech1', 't_speech2', 't_acous1', 't_acous2', 't_ins1', 't_ins2', 't_live1', 't_live2', 't_val1', 't_val2', 't_tempo1', 't_tempo2', 't_sig1', 't_sig2'])\n",
        "\n",
        "# Project only the necessary columns from each feature dataset and also rename to maintain consistency\n",
        "all_songs_with_features = pd.concat([df_song_features_1[['title', 'artist']], df_song_features_2[['title', 'artist']], \n",
        "                                     df_song_features_3[['title', 'artist']], df_song_features_4[['title', 'artist']], \n",
        "                                     df_song_features_5[['title', 'artist']], df_song_features_6[['title', 'artist']], \n",
        "                                     df_song_features_7[['title', 'artist']], df_song_features_8[['title', 'artist']], \n",
        "                                     df_song_features_9[['title', 'artist']], df_song_features_10[['title', 'artist']], \n",
        "                                     df_song_features_11[['title', 'artist']], df_song_features_12[['title', 'artist']]], ignore_index=True).drop_duplicates()\n",
        "\n",
        "# Join on song title and artist name\n",
        "feature_matches = pd.merge(df_charts_songs_artists, all_songs_with_features, on=['title', 'artist'], how='inner')\n",
        "\n",
        "# Calculate the percentage of songs each features dataset was able to provide song features for\n",
        "match_percentage_combined = 100 * feature_matches.shape[0] / df_charts_songs_artists.shape[0]\n",
        "print(\"We are able to provide audio features for \" + str(round(match_percentage_combined,2)) + \"% of songs in df_charts,\"\\\n",
        "      \" for a total of \" + str(feature_matches.shape[0]) + \" out of \" + str(df_charts_songs_artists.shape[0]) + \" songs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWfoMYNn0xcx"
      },
      "source": [
        "Lets now see whether we can do any additional cleaning to boost our match percentage. Since there are many obscure songs in `df_charts` that have, say, reached chart position 200 in small countries like Latvia and Ecuador, let us limit our search to songs that charted highly in the United States, and see if any of those songs were unmatched in the features data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AAL49Tp0xcx"
      },
      "outputs": [],
      "source": [
        "non_matches = pd.merge(df_charts[df_charts['rank'] > 10][df_charts['region'] == 'United States'][['title', 'artist']], \n",
        "                                  all_songs_with_features, on=['title', 'artist'], how='left', indicator=True).drop_duplicates()\n",
        "non_matches = non_matches[non_matches['_merge'] == 'left_only']\n",
        "print(non_matches[non_matches['artist'] == 'Roddy Ricch, NLE Choppa'])  # an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAkQD5ey0xcx"
      },
      "outputs": [],
      "source": [
        "# Does all_songs_with_features contain any of these songs but perhaps with a different format for title and artist?\n",
        "print(all_songs_with_features[all_songs_with_features['artist'] == 'NLE Choppa, Roddy Ricch'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYxPCiZ00xcx"
      },
      "source": [
        "This suggests that when there is a featured artist, the ordering of the artist names in the features datasets may be the opposite of the ordering in the charts dataset. Let's look at another:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_matches[non_matches['title'] == 'Cold Water (feat. Justin Bieber & MØ)'])"
      ],
      "metadata": {
        "id": "gOr437aJgkcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_songs_with_features[all_songs_with_features['artist'] == 'Major Lazer, Justin Bieber, MØ'])"
      ],
      "metadata": {
        "id": "QXtcp62fhK8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the featured artist is not included in the 'artist' column in the charts dataset, but is in the features datasets. In joins so far, we have been joining on 'artist' and 'title' to be sure to uniquely identify each song. But since these songs with featured artists have varying formats in the 'artist' column, but the same format in the 'title' column, and since they are sure to be uniquely identified just by song title, let us try to filter by songs that have \"(feat. \" in their title, and join the charts and features datasets on 'title'."
      ],
      "metadata": {
        "id": "fsPVjLKpgjkY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvySHSIt0xcy"
      },
      "outputs": [],
      "source": [
        "charts_featured = df_charts_songs_artists[df_charts_songs_artists['title'].str.contains(\"\\(feat\\. \")][['title', 'artist']]\n",
        "featured_feature_matches = pd.merge(charts_featured, all_songs_with_features, on=['title'], how='inner').drop_duplicates(subset='title')\n",
        "featured_feature_matches = featured_feature_matches.rename(columns={'artist_x': 'artist'})\n",
        "feature_matches = pd.concat([feature_matches, featured_feature_matches], ignore_index=True).drop_duplicates(subset = ['title', 'artist'])\n",
        "match_percentage_combined = 100 * feature_matches.shape[0] / df_charts_songs_artists.shape[0]\n",
        "print(\"We are able to provide audio features for \" + str(round(match_percentage_combined,2)) + \"% of songs in df_charts,\"\\\n",
        "      \" for a total of \" + str(feature_matches.shape[0]) + \" out of \" + str(df_charts_songs_artists.shape[0]) + \" songs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPvnrg5reeu2"
      },
      "source": [
        "There is one more case that might result in two rows for the same song in different dataframes not to match. We can see that the song title is always the same for the same song between datasets, but we get into trouble when multiple artists are on the same song. This need not be when there is a \"featured artist\". So let us match on song title and when `all_songs_with_features['artist']` contains one artist that is contained as a substring in the artists in df_charts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL-7GJBxCd6z"
      },
      "outputs": [],
      "source": [
        "# Match songs between charts and features datasets by checking whether one artist is contained as a substring in the artist field of the other\n",
        "feature_charts_substr_match = pd.merge(df_charts_songs_artists, all_songs_with_features, on=['title'], how='inner')\n",
        "feature_charts_substr_match['actual_match'] = feature_charts_substr_match[['artist_x', 'artist_y']].apply(lambda x: x['artist_y'].split(',')[0] in x['artist_x'] and x['artist_y'] != x['artist_x'], axis=1)\n",
        "feature_charts_substr_match = feature_charts_substr_match[feature_charts_substr_match['actual_match'] == True].rename(columns={'artist_x': 'artist'})\n",
        "feature_charts_substr_match.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D2xWBXqIDZI"
      },
      "source": [
        "We can see here many songs that aren't necessarily \"featured artist\" songs, that have different formats in the artist column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNLb9q4XITUR"
      },
      "outputs": [],
      "source": [
        "feature_charts_substr_match = feature_charts_substr_match.drop_duplicates(subset=['title', 'artist'])\n",
        "feature_matches = pd.concat([feature_matches, feature_charts_substr_match], ignore_index=True).drop_duplicates()\n",
        "feature_matches.drop(columns=['actual_match'], inplace=True)\n",
        "match_percentage_combined = 100 * feature_matches.shape[0] / df_charts_songs_artists.shape[0]\n",
        "print(\"We are able to provide audio features for \" + str(round(match_percentage_combined,2)) + \"% of songs in df_charts,\"\\\n",
        "      \" for a total of \" + str(feature_matches.shape[0]) + \" out of \" + str(df_charts_songs_artists.shape[0]) + \" songs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-MiLcop0xcy"
      },
      "source": [
        "Next, let's see what columns each of the features datasets share, and create one unified features dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMiBMVWZD4xt"
      },
      "outputs": [],
      "source": [
        "all_features_dfs = [df_song_features_1, df_song_features_2, df_song_features_3, df_song_features_4, df_song_features_5, df_song_features_6,\n",
        "                    df_song_features_7, df_song_features_8, df_song_features_9, df_song_features_10, df_song_features_11, df_song_features_12]\n",
        "for df in all_features_dfs:\n",
        "    print([col for col in df.columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3RIRHuiRq0E"
      },
      "source": [
        "Common columns:\n",
        "\n",
        "title, artist, tempo, energy, danceability, liveness, valence, duration, acousticness, speechiness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbIKYoFzPD4e"
      },
      "outputs": [],
      "source": [
        "features_1_rename_map = {}\n",
        "features_2_rename_map = {'bpm': 'tempo', 'nrgy': 'energy', 'dnce': 'danceability', 'live': 'liveness', 'val': 'valence', 'dur': 'duration_ms', 'acous': 'acousticness', 'spch': 'speechiness'}\n",
        "features_3_rename_map = {'Danceability': 'danceability', 'Energy': 'energy', 'Speechiness': 'speechiness', 'Acousticness': 'acousticness', 'Liveness': 'liveness', 'Tempo': 'tempo', 'Duration (ms)': 'duration_ms', 'Valence': 'valence'}\n",
        "features_4_rename_map = {}\n",
        "features_5_rename_map = {}\n",
        "features_6_rename_map = {}\n",
        "features_7_rename_map = {'t_dance0': 'danceability', 't_energy0': 'energy', 't_speech0': 'speechiness', 't_acous0': 'acousticness', 't_live0': 'liveness', 't_tempo0': 'tempo', 't_dur0': 'duration_ms', 't_val0': 'valence'}\n",
        "features_8_rename_map = {'t_dance1': 'danceability', 't_energy1': 'energy', 't_speech1': 'speechiness', 't_acous1': 'acousticness', 't_live1': 'liveness', 't_tempo1': 'tempo', 't_dur1': 'duration_ms', 't_val1': 'valence'}\n",
        "features_9_rename_map = {'t_dance2': 'danceability', 't_energy2': 'energy', 't_speech2': 'speechiness', 't_acous2': 'acousticness', 't_live2': 'liveness', 't_tempo2': 'tempo', 't_dur2': 'duration_ms', 't_val2': 'valence'}\n",
        "features_10_rename_map = {'t_dance0': 'danceability', 't_energy0': 'energy', 't_speech0': 'speechiness', 't_acous0': 'acousticness', 't_live0': 'liveness', 't_tempo0': 'tempo', 't_dur0': 'duration_ms', 't_val0': 'valence'}\n",
        "features_11_rename_map = {'t_dance1': 'danceability', 't_energy1': 'energy', 't_speech1': 'speechiness', 't_acous1': 'acousticness', 't_live1': 'liveness', 't_tempo1': 'tempo', 't_dur1': 'duration_ms', 't_val1': 'valence'}\n",
        "features_12_rename_map = {'t_dance2': 'danceability', 't_energy2': 'energy', 't_speech2': 'speechiness', 't_acous2': 'acousticness', 't_live2': 'liveness', 't_tempo2': 'tempo', 't_dur2': 'duration_ms', 't_val2': 'valence'}\n",
        "\n",
        "rename_maps = [features_1_rename_map, features_2_rename_map, features_3_rename_map, features_4_rename_map, features_5_rename_map, features_6_rename_map, \n",
        "               features_7_rename_map, features_8_rename_map, features_9_rename_map, features_10_rename_map, features_11_rename_map, features_12_rename_map]\n",
        "\n",
        "for i in range(len(all_features_dfs)):\n",
        "    all_features_dfs[i] = all_features_dfs[i].rename(columns=rename_maps[i])[['title', 'artist', 'duration_ms', 'tempo', 'energy', 'danceability', 'liveness', 'valence', 'acousticness', 'speechiness']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UIbeQP8YLb7"
      },
      "source": [
        "Check that all features dataframes have the same columns now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZnIVmsiX0nH"
      },
      "outputs": [],
      "source": [
        "for df in all_features_dfs:\n",
        "    print([col for col in df.columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxDepA63aIyk"
      },
      "source": [
        "Now let's concatenate all of these features dataframes into one and do an inner join with `feature_matches`, the songs with features that are in `df_charts` to get the final, combined features dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeAbE3bc3Csd"
      },
      "outputs": [],
      "source": [
        "df_features = pd.concat(all_features_dfs, axis=0, ignore_index=True)\n",
        "df_features = df_features.dropna()\n",
        "df_features_artist_format_1 = pd.merge(df_features, feature_matches, left_on=['title', 'artist'], right_on=['title', 'artist_y'], how='inner', suffixes=(\"\", \"_x\"))  # 'artist' in the format of df_charts\n",
        "df_features_artist_format_2 = pd.merge(df_features, feature_matches, on=['title', 'artist'], how='inner')  # 'artist' in the format of features dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGdMFMeo3G07"
      },
      "outputs": [],
      "source": [
        "df_features_artist_format_1.drop_duplicates(subset=['title', 'artist_y'], inplace=True)\n",
        "df_features_artist_format_1.drop(columns=['artist', 'artist_y'], inplace=True)\n",
        "df_features_artist_format_1.rename(columns={'artist_x': 'artist'}, inplace=True)\n",
        "df_features_artist_format_2.drop_duplicates(subset=['title', 'artist'], inplace=True)\n",
        "df_features_artist_format_2.drop(columns=['artist_y'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJpTdjHdHE7u"
      },
      "outputs": [],
      "source": [
        "df_features_artist_format_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBjP9e3mG9jj"
      },
      "outputs": [],
      "source": [
        "df_features_artist_format_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7Za_gGmJIgo"
      },
      "outputs": [],
      "source": [
        "df_features = pd.concat([df_features_artist_format_2, df_features_artist_format_1]).drop_duplicates(subset=['title', 'artist'])\n",
        "df_features.reset_index(drop=True, inplace=True)\n",
        "print(\"The number of songs in df_charts we have been able to find features for is: \" + str(df_features.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKkKazjVJuzF"
      },
      "outputs": [],
      "source": [
        "df_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwxDVqRDW2Sn"
      },
      "source": [
        "Join `df_features` with `df_charts` to get a unified dataframe with chart and feature columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJhTJokQK0EB"
      },
      "outputs": [],
      "source": [
        "df_charts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMB-dttiU6wV"
      },
      "outputs": [],
      "source": [
        "# df_charts_features is all unique songs that we have audio features for, and their highest rank in all territories in all years\n",
        "df_charts_features = pd.merge(df_charts.drop(columns=['date', 'region', 'chart']).groupby(by=['title', 'artist'], as_index=False).min(), df_features, on=['title', 'artist'], how='inner')\n",
        "# check that the number of songs is still the same after the merge, indicating that we did it correctly\n",
        "print(\"We have been able to find audio features for \" + str(df_charts_features.shape[0]) + \" unique songs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxTdy918VcXp"
      },
      "outputs": [],
      "source": [
        "# another inspection\n",
        "df_charts_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb6zsGiE0J3O"
      },
      "source": [
        "## Artist Popularity dataset feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOe8iwkI6GqB"
      },
      "source": [
        "We will want to explore whether an artist's past success is a predictor of current success. In order to do so, we can count how many days each artist spent on the charts in each year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hnoQUve0Pls"
      },
      "outputs": [],
      "source": [
        "# Copy Artist and Year from main df_charts_features table, one record per artist/Year combination\n",
        "df_trend_ranking = df_charts_features[['artist','Year']].drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the highest rank and song for each artist in each year\n",
        "max_ranking_df = df_charts[['artist', 'Year', 'rank']].groupby(['artist', 'Year'], as_index=False).min('rank')"
      ],
      "metadata": {
        "id": "j5IZ54nkfRYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMhmLHhi0PpK"
      },
      "outputs": [],
      "source": [
        "# Add Best Ranking Position in all regions\n",
        "max_ranking_df[['Year']] = max_ranking_df[['Year']].apply(lambda x: x + 1)  # because we want to know each artist's best ranking position in the previous year\n",
        "df_trend_ranking = df_trend_ranking.merge(max_ranking_df, left_on=['artist','Year'], right_on=['artist','Year'], how='left')\n",
        "df_trend_ranking.rename(columns={'rank':'Best_Ranking_Position'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxRsv3kg0Psp"
      },
      "outputs": [],
      "source": [
        "# Calculate number of days trending in the US and in Rest of the World\n",
        "days_trending_us = df_charts[['date', 'artist', 'Year']][df_charts['region']=='United States'].groupby(['artist', 'Year'], as_index=False)[['date']].count()\n",
        "days_trending_us[['Year']] = days_trending_us[['Year']].apply(lambda x: x + 1)\n",
        "days_trending_row = df_charts[['date', 'artist', 'Year']][df_charts['region']!='United States'].groupby(['artist', 'Year'], as_index=False)[['date']].count()\n",
        "days_trending_row[['Year']] = days_trending_row[['Year']].apply(lambda x: x + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzBKuUDN0PwE"
      },
      "outputs": [],
      "source": [
        "# Add days trending to df_trend_ranking\n",
        "df_trend_ranking = df_trend_ranking.merge(days_trending_us, left_on=['artist','Year'], right_on=['artist','Year'], how=\"left\")\n",
        "df_trend_ranking.rename(columns={'date':'Days_Trending_US'}, inplace=True)\n",
        "df_trend_ranking = df_trend_ranking.merge(days_trending_row, left_on=['artist','Year'], right_on=['artist','Year'], how=\"left\")\n",
        "df_trend_ranking.rename(columns={'date':'Days_Trending_ROW', 'Year_y':'Year'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbyBt71-0ZiO"
      },
      "outputs": [],
      "source": [
        "# Replace NaN for 0\n",
        "df_trend_ranking[['Days_Trending_US', 'Days_Trending_ROW']] = df_trend_ranking[['Days_Trending_US', 'Days_Trending_ROW']].fillna(0)\n",
        "# fill with 201 because 0 would be interpreted during modelling as \"better\" than rank 1.\n",
        "df_trend_ranking[['Best_Ranking_Position']] = df_trend_ranking[['Best_Ranking_Position']].fillna(201)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNt5uxJD0Zsv"
      },
      "outputs": [],
      "source": [
        "# Final table with artist, top position across all regions, and days trending in the US and ROW\n",
        "del df_charts\n",
        "del max_ranking_df\n",
        "del days_trending_us\n",
        "del days_trending_row\n",
        "gc.collect()\n",
        "df_trend_ranking.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9uFxr2sQGxY"
      },
      "source": [
        "## TikTok Popular Songs feature engineering and EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TikTok has become a growing force in the music industry, with many artists becoming big after their songs were used in viral videos on TikTok. We are interested in whether data about the most popular songs on TikTok can help us predict a song's success on Spotify better, and how much of an influence TikTok popularity has on a song's success on Spotify."
      ],
      "metadata": {
        "id": "JkWCqq39jt1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we reviewed the dataframes to make sure that they had the same structure. "
      ],
      "metadata": {
        "id": "Y3yoYhNA-vh1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-JdIrzO08GS"
      },
      "outputs": [],
      "source": [
        "df_tiktok_19.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9txjEQY71b4g"
      },
      "outputs": [],
      "source": [
        "df_tiktok_20.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhbEd8of1cI5"
      },
      "outputs": [],
      "source": [
        "df_tiktok_21.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we combined the datasets. "
      ],
      "metadata": {
        "id": "Wp6NUDzm-4zE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKJBDGC21cZF"
      },
      "outputs": [],
      "source": [
        "# Combine the tiktok datasets\n",
        "data_tiktok = [df_tiktok_19, df_tiktok_20, df_tiktok_21]\n",
        "df_tiktok_full = pd.concat(data_tiktok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AXhyqlrMnYp"
      },
      "outputs": [],
      "source": [
        "# Extract the track name and artist to check for the overlap\n",
        "df_tiktok_tracks = df_tiktok_full[['track_name', 'artist_name']]\n",
        "\n",
        "# Check number of rows\n",
        "df_tiktok_tracks.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next the tiktok data was merged with the features dataframe. Matching was done on title and a softer matching on artist because we saw variation in the artist columns, e.g. two artists for a song but recorded in a different order in the wtow dataframes. "
      ],
      "metadata": {
        "id": "v7TM2Wqk_cAa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBAyfruiUPD1"
      },
      "outputs": [],
      "source": [
        "# Merge with spotify features on track names only to check overlap\n",
        "df_tiktok_tracks_only = feature_matches.merge(df_tiktok_tracks.drop_duplicates(), left_on = ['title'], right_on=['track_name'], how='inner')\n",
        "\n",
        "# Check overlap\n",
        "df_tiktok_tracks_only.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C0fFJATUy4M"
      },
      "outputs": [],
      "source": [
        "# Review rows that didn't match to see if there's more clean-up that can be done \n",
        "df_tiktok_full_merge = feature_matches.merge(df_tiktok_tracks.drop_duplicates(), left_on = ['title'], right_on=['track_name'], how='outer', indicator = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnSpyq4oV8bu"
      },
      "outputs": [],
      "source": [
        "df_tiktok_full_merge.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BUzsCr2VuXv"
      },
      "outputs": [],
      "source": [
        "# Review rows that didn't match to see if there's more clean-up that can be done \n",
        "df_tiktok_full_merge[df_tiktok_full_merge['_merge'] != 'both']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wxuu_bf2WYfF"
      },
      "outputs": [],
      "source": [
        "# do a check to see if converting the track names to lower case could get more matches \n",
        "df_tiktok_full_merge.dropna(inplace = True)\n",
        "df_tiktok_full_merge['title_l'] = df_tiktok_full_merge.apply(lambda x : x['title'].lower(), axis =1)\n",
        "df_tiktok_full_merge['track_name_l'] = df_tiktok_full_merge.apply(lambda x : x['track_name'].lower(), axis =1)\n",
        "\n",
        "print(df_tiktok_full_merge[(df_tiktok_full_merge['title_l'] == df_tiktok_full_merge['track_name_l']) & (df_tiktok_full_merge['_merge'] != 'both')])\n",
        "\n",
        "# prints an empty dataframe so the conclusion is no"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evjfggiCWtcD"
      },
      "outputs": [],
      "source": [
        "# Create a match column to indicate cases where the tiktok artist name can be found as a substring in the spotify artist column \n",
        "df_tiktok_tracks_only['match'] = df_tiktok_tracks_only.apply(lambda x: x['artist_name'].lower().find(x['artist'].lower()), axis=1).ge(0)\n",
        "\n",
        "# Create a match column to indicate cases where the spotify artist column can be found as a substring in the tiktok artist name \n",
        "df_tiktok_tracks_only['match_2'] = df_tiktok_tracks_only.apply(lambda x: x['artist'].lower().find(x['artist_name'].lower()), axis=1).ge(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef71rA5qiRYD"
      },
      "outputs": [],
      "source": [
        "# review data to see which rows have matched\n",
        "df_tiktok_tracks_only[df_tiktok_tracks_only['match'] | df_tiktok_tracks_only['match_2']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9khMP9ZTkFg"
      },
      "outputs": [],
      "source": [
        "# review data to see which rows have not matched\n",
        "df_tiktok_tracks_only[(df_tiktok_tracks_only['match'] == False) & (df_tiktok_tracks_only['match_2'] == False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMJjLZcVijGK"
      },
      "outputs": [],
      "source": [
        "# check the rows where the artists names aren't an exact match - for visual inspection \n",
        "df_tiktok_tracks_only.loc[(df_tiktok_tracks_only['artist'] != df_tiktok_tracks_only['artist_name']) & (df_tiktok_tracks_only['match'] |df_tiktok_tracks_only['match_2'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VbUNXhYjJKW"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe of overlapping songs based on the two matches colums and drop duplicates\n",
        "df_tiktok_matches_final = df_tiktok_tracks_only.loc[(df_tiktok_tracks_only['match'] |df_tiktok_tracks_only['match_2'])]\n",
        "\n",
        "# Check the shape of the df \n",
        "df_tiktok_matches_final.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbwjCLEOTJzD"
      },
      "outputs": [],
      "source": [
        "# spot check an artist to see if the name matching looks appropriate \n",
        "df_tiktok_matches_final.loc[(df_tiktok_matches_final['artist_name'] == \"Justin Bieber\" )|( df_tiktok_matches_final['artist'] == \"Justin Bieber\" )]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrdLP4IVPxZZ"
      },
      "outputs": [],
      "source": [
        "# drop rows that are no longer needed\n",
        "df_tiktok_matches_final.drop(['match', 'match_2', 'track_name', 'artist_name','artist_y'], inplace =True, axis=1)\n",
        "\n",
        "# add a column to indicate tiktok popularity \n",
        "df_tiktok_matches_final= df_tiktok_matches_final.assign(tiktok_pop = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSScxkw6RHrq"
      },
      "outputs": [],
      "source": [
        "df_tiktok_matches_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7ZBdyHGRaib"
      },
      "outputs": [],
      "source": [
        "df_tiktok_matches_final.groupby(['tiktok_pop']).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataframe wtih tikok data and features were now combined. "
      ],
      "metadata": {
        "id": "zkPYlOcI_tMK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg0m6--cHd5E"
      },
      "outputs": [],
      "source": [
        "df_charts_features_tiktok = pd.merge(df_charts_features, df_tiktok_matches_final, on=['title', 'artist'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v__LJn_w1Hg"
      },
      "outputs": [],
      "source": [
        "df_charts_features_tiktok['tiktok_pop'] = df_charts_features_tiktok.apply(lambda x: 1 if x['tiktok_pop'] == 1 else (0 if x['Year'] > 2018 else x['tiktok_pop']), axis=1)\n",
        "missing_rows = df_charts_features_tiktok[df_charts_features_tiktok['tiktok_pop'].isna()]\n",
        "missing_rows.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGKnP7QFivsE"
      },
      "source": [
        "### Value imputation\n",
        "\n",
        "We'll impute values for tiktok popularity for the years 2017 and 2018 based on the data we have for 2019-2021. We'll use KNN for this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deAtbZ1hi_JX"
      },
      "outputs": [],
      "source": [
        "# create a KNN imputer object\n",
        "imputer = KNNImputer(n_neighbors=3)\n",
        "# impute missing values in the 'tiktok_pop' column\n",
        "df_charts_features_tiktok['tiktok_pop'] = imputer.fit_transform(df_charts_features_tiktok[['tiktok_pop']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd4p3cWUkwds"
      },
      "outputs": [],
      "source": [
        "df_charts_features_tiktok.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fix any unexpected values that may have been imputed\n",
        "df_charts_features_tiktok['tiktok_pop'] = df_charts_features_tiktok.apply(lambda x: 1 if x['tiktok_pop'] > 0.5 else 0, axis=1)"
      ],
      "metadata": {
        "id": "aI1nCPlABCiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_charts_features_tiktok.head()"
      ],
      "metadata": {
        "id": "8AaEWa67HpNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8FJ7BeRmGjV"
      },
      "source": [
        "Check if we've successfully removed null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XVLMbPnmD12"
      },
      "outputs": [],
      "source": [
        "missing_rows = df_charts_features_tiktok[df_charts_features_tiktok['tiktok_pop'].isna()]\n",
        "print(missing_rows.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZSgQ1LIlGOZ"
      },
      "source": [
        "We'll inspect to see if we have imputed values for 2017 and 2018."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtXVP-LQlFVY"
      },
      "outputs": [],
      "source": [
        "df_charts_features_tiktok[(df_charts_features_tiktok['tiktok_pop'] == 1) & (df_charts_features_tiktok['Year'] == 2018)][0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_r20MQtldbl"
      },
      "outputs": [],
      "source": [
        "df_charts_features_tiktok[(df_charts_features_tiktok['tiktok_pop'] == 1) & (df_charts_features_tiktok['Year'] == 2017)][0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obaeh7xLtXyu"
      },
      "source": [
        "Lets inspect the dataframe of merged charts, features and tiktok data to see if there is any final cleaning to do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Gw64drtHteM"
      },
      "outputs": [],
      "source": [
        "df_charts_features_tiktok[(df_charts_features_tiktok['tiktok_pop'] == 1) & (df_charts_features_tiktok['artist'] == 'Ariana Grande')][0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SAJdDSItl3g"
      },
      "source": [
        "We can see that some rows have features that are formatted differently than the rest. If you expand the dataframe you can see that there are many more rows like that. These rows are formatted as percentages rather than decimals between 0 and 1, so we can run an apply function to change the format of any such row. We can also see that `duration_ms` is formatted as seconds in such rows, so we should edit that as well. And finally we can also see that some rows have empty features values, so we should drop those."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbYLY_W-uFp9"
      },
      "outputs": [],
      "source": [
        "# drop rows with emtpy features values\n",
        "mask = df_charts_features_tiktok['energy'].str.strip() == ''\n",
        "df_charts_features_tiktok = df_charts_features_tiktok[~mask].copy()\n",
        "\n",
        "# convert formats of features columns with percentages to floats between 0 and 1\n",
        "cols = ['danceability',\t'energy',\t'speechiness',\t'acousticness', 'liveness',\t'valence']\n",
        "for col in cols:\n",
        "  df_charts_features_tiktok[col] = df_charts_features_tiktok[col].apply(lambda x: float(x) / 100 if float(x) > 1 else float(x))\n",
        "\n",
        "# convert format of duration columns in seconds to ms\n",
        "df_charts_features_tiktok['duration_ms'] = df_charts_features_tiktok['duration_ms'].apply(lambda x: int(x) * 1000 if int(x) < 10000 else int(x))\n",
        "df_charts_features_tiktok[(df_charts_features_tiktok['tiktok_pop'] == 1) & (df_charts_features_tiktok['artist'] == 'Ariana Grande')][0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGSjOsLo7nvr"
      },
      "source": [
        "And we can see that there are still some duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khYMVN3o7sRK"
      },
      "outputs": [],
      "source": [
        "df_charts_features_tiktok.drop_duplicates(inplace=True)\n",
        "df_charts_features_tiktok.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGKlQeaooOCb"
      },
      "source": [
        "Convert tempo column to float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNmy8IR_nrSX"
      },
      "outputs": [],
      "source": [
        "df_charts_features_tiktok['tempo'] = df_charts_features_tiktok['tempo'].apply(lambda x: float(x))\n",
        "df_charts_features_tiktok['tempo'] = df_charts_features_tiktok['tempo'].astype('float64')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we look at the relationship between tiktok data and other features. "
      ],
      "metadata": {
        "id": "-XQpRggsCGF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tiktok_matrix =df_charts_features_tiktok.drop(['artist','title'], axis=1)"
      ],
      "metadata": {
        "id": "XLnhjYBnCNb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create scatter matrix with only one row and include sepal_width, petal_length, and petal_width features\n",
        "sns.pairplot(df_tiktok_matrix, x_vars=['tiktok_pop'], height=3, aspect=1.5, plot_kws={'s': 10})\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gm9Y3gGtCqXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see interesting relationships between tiktok popularity and other features:\n",
        "- songs that are popular on tiktok tend to have higher energy and danceability \n",
        "- they have fewer words\n",
        "- they are shorter "
      ],
      "metadata": {
        "id": "UF3sz7NdAGxv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eekt5MsyjswP"
      },
      "source": [
        "## Genius Lyrics dataset creation, EDA and feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P27l5_jRw22"
      },
      "source": [
        "We want to find lyrics for our songs so that we can include these in our dataset. \n",
        "\n",
        "The [lyricsgenius](https://) library will be used. This library makes it easier to extract lyrics quickly from the lyrics website [Genius](https://). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9YN6Cl5UfYN"
      },
      "source": [
        "To use the lyricsgenius library we needed to sign-up for an account that authorizes access to the Genius API. The Genius account provides a access_token that is required by the lyricsgenius package. The cell below includes the required credentials.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37JKFZI3n8Fu"
      },
      "outputs": [],
      "source": [
        "client_id = 'TS15U5iwbWLGkhfGFVqnOuDA9mVjJhhLlXpJDYai6nm79S9JWFzznlsQN5dCFuZG'\n",
        "client_secret = 'SOhrXQxD9YZ2RxBQwR-wwu5Zbxh6UgkfuIEaUJltx9L9h8aynN5zZ9Jsm1JNlh_5Npu_uev1MKorJV_A6MVYKw'\n",
        "access_token = 'US00oQ6_8lkhwRjHAOudB62bruf1B3JGOGuHR8V8zeawxpoc8fcA9QGXQ3bhYWu-'\n",
        "website_url = 'https://github.com/Joseph-Willem-Ricci/5450-final-project'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CW70cN2Uz01"
      },
      "source": [
        "It wasn't possible to use the lyricsgenius package directly from colab because the connection was blocked due to security restrictions. Instead, the code below was run locally to extract lyrics for the songs that we had in the final\n",
        "`df_charts_features` dataframe. \n",
        "\n",
        "```python\n",
        "!pip install lyricsgenius\n",
        "import lyricsgenius\n",
        "import os\n",
        "import spacy\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "client_id = 'TS15U5iwbWLGkhfGFVqnOuDA9mVjJhhLlXpJDYai6nm79S9JWFzznlsQN5dCFuZG'\n",
        "client_secret = 'SOhrXQxD9YZ2RxBQwR-wwu5Zbxh6UgkfuIEaUJltx9L9h8aynN5zZ9Jsm1JNlh_5Npu_uev1MKorJV_A6MVYKw'\n",
        "access_token = 'US00oQ6_8lkhwRjHAOudB62bruf1B3JGOGuHR8V8zeawxpoc8fcA9QGXQ3bhYWu-'\n",
        "website_url = 'https://github.com/Joseph-Willem-Ricci/5450-final-project'\n",
        "\n",
        "# pull in the csv of songs to search for\n",
        "df_combined_features = pd.read_csv('combined_features.csv')\n",
        "\n",
        "# initialise the API, make sure that it will retry if a request fails, remove headers that \n",
        "# demarcate the song, e.g. 'chorus', 'verse'\n",
        "api = lyricsgenius.Genius(access_token, retries = 500, remove_section_headers = True)\n",
        "\n",
        "\n",
        "# initialise a dataframe to store the results\n",
        "all_song_data = pd.DataFrame()\n",
        "\n",
        "# track time because this query takes a long time to run, e.g. 24 hours \n",
        "start_time = datetime.now()\n",
        "\n",
        "# as we loop through and search, print some info about progress because it's going to run for so long\n",
        "print(\"Started at {}\".format(start_time))\n",
        "for i in range((1924+3678), len(df_combined_features)):\n",
        "    rolling_pct = int((i/len(df_combined_features))*100)\n",
        "    print(str(rolling_pct) + \"% complete.\" + \" Collecting Record \" + str(i) +\" of \" +\n",
        "          str(len(df_combined_features))+ \".\" + \" Currently collecting \" + \n",
        "          df_combined_features.iloc[i]['title'] + \" by \" + df_combined_features.iloc[i]['artist'] + \" \"*50, end=\"\\r\")\n",
        "    \n",
        "    # store the song information that we already have in our csv\n",
        "    song_title = df_combined_features.iloc[i]['title']\n",
        "    song_title = re.sub(\" and \", \" & \", song_title)\n",
        "    artist_name = df_combined_features.iloc[i]['artist']\n",
        "    artist_name = re.sub(\" and \", \" & \", artist_name)\n",
        "\n",
        "    # search for the lyrics and song url, we don't want a crash if a song's details can't be found so use a try catch block\n",
        "    try:\n",
        "        song = api.search_song(song_title, artist=artist_name)\n",
        "        song_lyrics = re.sub(\"\\n\", \" \", song.lyrics) #Remove newline breaks, we won't need them.\n",
        "        song_url = song.url\n",
        "    except:\n",
        "        song_lyrics = \"null\"\n",
        "        song_url = \"null\"\n",
        "\n",
        "    # create a row in the dataframe with the final info that we found for a song    \n",
        "    row = {\n",
        "        \"title\": df_combined_features.iloc[i]['title'],\n",
        "        \"artist\": df_combined_features.iloc[i]['artist'],\n",
        "        \"lyrics\": song_lyrics,\n",
        "        \"song URL\": song_url,\n",
        "    }\n",
        "\n",
        "    # add a row to the dataframe\n",
        "    all_song_data = all_song_data.append(row, ignore_index=True)\n",
        "\n",
        "    # update csv - decided to do this for every song incase there was a crash during the run\n",
        "    all_song_data.to_csv(\"all_song_data_final.csv\")\n",
        "\n",
        "# print out final time to run\n",
        "end_time = datetime.now()\n",
        "print(\"\\nCompleted at {}\".format(start_time))\n",
        "print(\"Total time to collect: {}\".format(end_time - start_time))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpb_IbuMW32i"
      },
      "source": [
        "This process resulted in multiple files being generated because I ran batches as we figured out how to expand our base data set. These were merged into a final CSV.\n",
        "\n",
        "The final CSV generated contained some bad quality data. I cleaned the file locally before uploading it to Kaggle to be imported into this notebook.\n",
        "\n",
        "The cleaning steps were determined by analysing the data extracted:\n",
        "1.   I noticed that some rows had no lyrics so these were dropped. \n",
        "2.   Some rows had odd URLs, when I looked these up I realised that they were articles about the song. I inspected to find a pattern that I could use to determine which rows should be deleted. In the end, it seemed best to compare the first two characters in the url with the first two characters of the artist's name. If they didn't match these rows were dropped. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK-ulbGvhIPd"
      },
      "source": [
        "I created a project on Kaggle and uploaded the lyrics csv to that. This could then be downloaded into this notebook.\n",
        "\n",
        "```python\n",
        "# merge the different CSVs\n",
        "df_lyrics_1 = pd.read_csv('all_song_data_final.csv', low_memory=False)\n",
        "df_lyrics_2 = pd.read_csv('all_song_data_final_2.csv', low_memory=False)\n",
        "df_lyrics_3 = pd.read_csv('all_song_data_final_3.csv', low_memory=False)\n",
        "df_lyrics_4 = pd.read_csv(\"all_song_data_final_4.csv\", low_memory=False)\n",
        "df_lyrics_5 = pd.read_csv(\"all_song_data_final_5.csv\", low_memory=False)\n",
        "\n",
        "lyrics_list = [df_lyrics_1, df_lyrics_2, df_lyrics_3,df_lyrics_4]\n",
        "df_lyrics = pd.concat(lyrics_list)\n",
        "\n",
        "# check size of dataframe\n",
        "print(df_lyrics.shape)\n",
        "\n",
        "# drop duplicate rows\n",
        "print(df_lyrics[df_lyrics['song URL'].isna()])\n",
        "df_lyrics.drop_duplicates(inplace = True)\n",
        "\n",
        "# check dataframe\n",
        "df_lyrics.head()\n",
        "\n",
        "# remove rows where the URL is incorrect - in some cases the lyrics was actually an article about the song \n",
        "# tried to find patterns before finally matching on first two letters\n",
        "\n",
        "# first thought that the strange rows always had the word 'annotated', 'chapter' or 'discography' in URL but this wasn't true\n",
        "print(df_lyrics[~df_lyrics['song URL'].str.contains('annotated')])\n",
        "print(df_lyrics[~df_lyrics['song URL'].str.contains('chapter')])\n",
        "print(df_lyrics_two = df_lyrics[~df_lyrics['song URL'].str.contains('discography')])\n",
        "\n",
        "# dropped unnecessary columns\n",
        "df_lyrics = df_lyrics[['title', 'artist', 'lyrics', 'song URL']]\n",
        "\n",
        "## created new columns for the URL and artist so that I could compare them  \n",
        "df_lyrics['clean URL'] = df_lyrics['song URL'].str.replace('-', '').str.lower()\n",
        "df_lyrics['clean artist'] = df_lyrics['artist'].str.replace(' ', '').str.lower()\n",
        "\n",
        "# extracted first two letters of artist from URL\n",
        "df_lyrics['URL letters'] = df_lyrics['clean URL'].str.extract(r'https://genius\\.com/(\\w{2})')\n",
        "\n",
        "# found rows where there was a match between artist and URL\n",
        "df_lyrics_two = df_lyrics.loc[df_lyrics['URL letters'] == df_lyrics['clean artist'].str[:2]]\n",
        "print(df_lyrics_two.head(50))\n",
        "\n",
        "# sense check by printing the rows that didn't match to see if my matching worked\n",
        "df_lyrics_three = df_lyrics.loc[df_lyrics['URL letters'] != df_lyrics['clean artist'].str[:2]]\n",
        "print(df_lyrics_three.head(50))\n",
        "\n",
        "# created the final csv to upload to Kaggle\n",
        "df_lyrics_two.to_csv(\"all_song_data_complete_V4.csv\", encoding=\"utf-8\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywy08Es-DH9D"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!!kaggle datasets download -d joannecrean/spotify-songs-lyrics\n",
        "!unzip /content/spotify-songs-lyrics.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JQACzkFhpHW"
      },
      "source": [
        "### Feature engineering: Sentiment analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdJy23nkhbVb"
      },
      "source": [
        "Sentiment analysis was run on the lyrics and this was stored in the final lyrics CSV. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPZfntq-CKy2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "stopwords = set(stopwords.words(['english', 'spanish', 'german','french']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z588WsRN0Sek"
      },
      "outputs": [],
      "source": [
        "df_lyrics = pd.read_csv('all_song_data_complete_V4.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrVgw4Kh1A9"
      },
      "source": [
        "I created a CSV of sensitive words that are often in songs but may be misintepreted easily. I ran this locally because I didn't want to write this list into my notebook as the words may offend the reader. The ```sensitive.py``` may be found the [project github repo](https://github.com/Joseph-Willem-Ricci/5450-final-project). The code run is below. \n",
        "\n",
        "This sensitive words CSV was also added to the github repo. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZ5P__YKtqWf"
      },
      "outputs": [],
      "source": [
        "sensitive_words = pd.read_csv('sensitive_words.csv')\n",
        "sensitive_words = sensitive_words.values.tolist()\n",
        "sensitive_words = [x[1] for x in sensitive_words]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm ./*.zip\n",
        "!rm ./*.csv"
      ],
      "metadata": {
        "id": "lERamyV1nszz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D31bE3Ll4l_P"
      },
      "source": [
        "```python\n",
        "sensitive_words = ['list of sensitive words']\n",
        "words_ser = pd.Series(sensitive_words)\n",
        "words_ser.to_csv(\"sensitive_words.csv\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFO_EiBGCUsd"
      },
      "outputs": [],
      "source": [
        "def tokenize_content(content):\n",
        "  content = content.lower()  # convert to lowercase\n",
        "  content_tokens = nltk.word_tokenize(content)  # tokenise\n",
        "  alpha_tokens = [w for w in content_tokens if w.isalpha()]  # keep alpha tokens\n",
        "  stop_tokens = [t for t in alpha_tokens if not t in stopwords]  # remove if stopword\n",
        "  final_tokens = [s for s in stop_tokens if not s in sensitive_words]  # remove if sensitive\n",
        "  return final_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxDXD3obDweO"
      },
      "outputs": [],
      "source": [
        "df_lyrics.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0PWn_-EECd_"
      },
      "outputs": [],
      "source": [
        "df_lyrics.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KblsyvnF9BHy"
      },
      "outputs": [],
      "source": [
        "# remove the intro text before the lyrics \n",
        "df_lyrics['lyrics'] = [re.sub('^.*Lyrics', '', x) for x in df_lyrics['lyrics']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJJGsOi-4gYy"
      },
      "outputs": [],
      "source": [
        "len(df_lyrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kXjFyE9apEY"
      },
      "outputs": [],
      "source": [
        "df_lyrics.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Rx6KfHECiu0"
      },
      "outputs": [],
      "source": [
        "df_lyrics['tokenized'] = df_lyrics.apply(lambda x: tokenize_content(x.lyrics), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2amPvUbLEIQg"
      },
      "outputs": [],
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "def retrieve_sentiment(content):\n",
        "  return sia.polarity_scores(content)['compound']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrWOeSiLEiDW"
      },
      "outputs": [],
      "source": [
        "translator = Translator(to_lang='en')  # translate all lyrics to english before calculating sentiment score\n",
        "df_lyrics['sentiment'] = df_lyrics.apply(lambda x: retrieve_sentiment(translator.translate(x.lyrics)), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfhsGP1-jMNN"
      },
      "source": [
        "When the sentiment column was added, check the most positive and negative songs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypdtMyrrjLGN"
      },
      "outputs": [],
      "source": [
        "df_lyrics_positive = df_lyrics.sort_values(by = ['sentiment'], ascending = False)\n",
        "df_lyrics_positive.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEU9M2g5jswR"
      },
      "outputs": [],
      "source": [
        "df_lyrics_negative = df_lyrics.sort_values(by = ['sentiment'])\n",
        "# the resulting df is commented out because of profanity and possibly offensive words\n",
        "#df_lyrics_negative.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OM_jWsV5V9o"
      },
      "outputs": [],
      "source": [
        "# group the data by genre and calculate the median sentiment for each group\n",
        "lyrics_sen = df_lyrics.groupby('sentiment').count().reset_index()\n",
        "lyrics_sen.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2PeNLx7JXPz"
      },
      "outputs": [],
      "source": [
        "# check number of unique artists and songs in the dataframe\n",
        "unique_artists = df_lyrics['artist'].nunique()\n",
        "unique_songs = df_lyrics['title'].nunique()\n",
        "\n",
        "print(\"unique artists:\", unique_artists)\n",
        "print(\"unique songs:\", unique_songs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HKKZlFu7AHm"
      },
      "outputs": [],
      "source": [
        "# group the data by genre and calculate the median sentiment for each group\n",
        "lyrics_sen = df_lyrics.groupby('sentiment')['title'].nunique().reset_index()\n",
        "lyrics_sen.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUeZva4a79s8"
      },
      "outputs": [],
      "source": [
        "df_lyrics['sentiment_gr'] = df_lyrics['sentiment'].round(1)\n",
        "lyrics_sen = df_lyrics.groupby('sentiment_gr')['title'].nunique().reset_index()\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "sns.barplot(x='sentiment_gr', y='title', data=lyrics_sen)\n",
        "plt.xticks(rotation = 45)\n",
        "\n",
        "# add a title and labels to the plot\n",
        "plt.title('Distribution of sentiment scores')\n",
        "plt.xlabel('sentiment score')\n",
        "plt.ylabel('number of songs')\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITHiUnEIE82l"
      },
      "outputs": [],
      "source": [
        "lyrics_sen_artist = df_lyrics.groupby('sentiment_gr')['artist'].nunique().reset_index()\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "sns.barplot(x='sentiment_gr', y='artist', data=lyrics_sen_artist)\n",
        "plt.xticks(rotation = 45)\n",
        "\n",
        "# add a title and labels to the plot\n",
        "plt.title('Distribution of sentiment scores')\n",
        "plt.xlabel('sentiment score')\n",
        "plt.ylabel('number of artists')\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftp7xwlfjWja"
      },
      "source": [
        "### Feature engineering: Extract words from lyrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS51N3rijdPB"
      },
      "source": [
        "I wanted to add a columns counting unique words and total words to the final ```df_lyrics``` as we were curious to see if these features impacted a song's success. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJD9L0s-uM-j"
      },
      "outputs": [],
      "source": [
        "#drop unnamed column\n",
        "df_lyrics.drop(columns = ['Unnamed: 0'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czuILLnVlRL7"
      },
      "source": [
        "Get the total word count and unique words for each song. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIa3VED9uTHu"
      },
      "outputs": [],
      "source": [
        "word_counts = []\n",
        "unique_word_counts = []\n",
        "for i in range (0, len(df_lyrics)):\n",
        "    word_counts.append(len(df_lyrics.iloc[i]['lyrics'].split()))\n",
        "    unique_word_counts.append(len(set(df_lyrics.iloc[i]['lyrics'].split())))\n",
        "df_lyrics['word counts'] = word_counts\n",
        "df_lyrics['unique word counts'] = unique_word_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE-DH95xlyOd"
      },
      "source": [
        "Create the final lyrics dataset for merging with other datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u6frIYHLoLY"
      },
      "outputs": [],
      "source": [
        "lyrics_sen_artist = df_lyrics.groupby('word counts')['title'].nunique().reset_index()\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.lineplot(x='word counts', y='title', data=lyrics_sen_artist)\n",
        "plt.xticks(rotation = 45)\n",
        "\n",
        "# add a title and labels to the plot\n",
        "plt.title('Distribution of word counts')\n",
        "plt.xlabel('word count')\n",
        "plt.ylabel('number of songs')\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K30W7-myOjbT"
      },
      "source": [
        "Review the number of songs that have a very high word count, we may need to drop these as outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-9Jn-pwMoXf"
      },
      "outputs": [],
      "source": [
        "df_lyrics_high_counts = df_lyrics[df_lyrics['word counts'] >= 1000]\n",
        "df_lyrics_high_counts.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T42uGxOPMyfo"
      },
      "outputs": [],
      "source": [
        "df_lyrics_high_counts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vMJC8ygPHx3"
      },
      "outputs": [],
      "source": [
        "df_lyrics_zero = df_lyrics[df_lyrics['word counts'] < 10]\n",
        "df_lyrics_zero.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpcFP8G2Ov9I"
      },
      "source": [
        "On inspection the lyrics in `df_lyrics_high_counts` don't seem like valid song lyrics, the API must have scraped more from some pages than just the lyrics, so we'll remove these rows. The lyrics in `df_lyrics_zero` seem to all include an extra piece of text \"You might also like...\", which we could clean from the lyrics column of each row, but we can also see that this phrase has no bearing on the sentiment, as evidenced by the bottom three rows, so while the lyrics column itself is not totally clean, since we'll only be using the sentiment column for analysis, and since that is clean, there is no need to clean the lyrics column of this extra string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RShgV2DpO6cy"
      },
      "outputs": [],
      "source": [
        "# drop outliers\n",
        "df_lyrics = df_lyrics[df_lyrics['word counts'] <= 2500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "towJaXMsQi7x"
      },
      "outputs": [],
      "source": [
        "lyrics_sen_artist = df_lyrics.groupby('word counts')['title'].nunique().reset_index()\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.lineplot(x='word counts', y='title', data=lyrics_sen_artist)\n",
        "plt.xticks(rotation = 45)\n",
        "\n",
        "# add a title and labels to the plot\n",
        "plt.title('Distribution of word counts')\n",
        "plt.xlabel('word count')\n",
        "plt.ylabel('number of songs')\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsMMaYuHyzHW"
      },
      "source": [
        "Create the final lyrics dataset for merging with other datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QY-m_yWCXJw"
      },
      "outputs": [],
      "source": [
        "df_lyrics_ready = df_lyrics[['title', 'artist', 'sentiment', 'sentiment_gr','word counts', 'unique word counts']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwfjeoGE8Klj"
      },
      "outputs": [],
      "source": [
        "df_lyrics_ready.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RZX6lNwo17Q"
      },
      "source": [
        "# Final dataset creation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMPcE3A3nUTA"
      },
      "source": [
        "The final datasets are merged together and inspected\n",
        "\n",
        "The datasets are:\n",
        "1.   `df_trend_ranking`\n",
        "2.   `df_lyrics_ready`\n",
        "3.   `df_charts_features_tiktok`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST OUTER JOIN WITH LYRICS TO SEE IF WE CAN IMPUT LYRIC VALUES\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_final = df_charts_features_tiktok.merge(df_lyrics_ready, left_on = ['title', 'artist'], right_on = ['title', 'artist'], how = 'left')\n",
        "df_final.drop_duplicates(inplace=True)\n",
        "df_final.loc[:, ['sentiment', 'sentiment_gr','word counts', 'unique word counts']] = imputer.fit_transform(df_final.loc[:, ['sentiment', 'sentiment_gr','word counts', 'unique word counts']])\n",
        "df_final.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "1XmdTO_A0otN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EB6_e7emmEt"
      },
      "outputs": [],
      "source": [
        "# df_final = df_charts_features_tiktok.merge(df_lyrics_ready, left_on = ['title', 'artist'], right_on = ['title', 'artist'], how = 'inner')\n",
        "# df_final.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Es4gcroo5dF"
      },
      "outputs": [],
      "source": [
        "print(len(df_final))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWUUnDnVo6wn"
      },
      "outputs": [],
      "source": [
        "df_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6NjvtaVopE2"
      },
      "outputs": [],
      "source": [
        "df_final = df_final.merge(df_trend_ranking, on = ['artist', 'Year'], how = 'left', indicator = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wNPwF7wnDtT"
      },
      "outputs": [],
      "source": [
        "df_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxMdMRcUnHzA"
      },
      "outputs": [],
      "source": [
        "print(len(df_final))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlCQ--mKHL3Y"
      },
      "outputs": [],
      "source": [
        "df_final['Best_Ranking_Position'] = df_final['Best_Ranking_Position'].fillna(0)\n",
        "df_final['Days_Trending_US'] = df_final['Days_Trending_US'].fillna(0)\n",
        "df_final['Days_Trending_ROW'] = df_final['Days_Trending_ROW'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "takyle9DnJlV"
      },
      "outputs": [],
      "source": [
        "df_final.info(verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhobDQ40TqBs"
      },
      "source": [
        "Tempo should be converted to `float64`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvIVz0ZATwC3"
      },
      "outputs": [],
      "source": [
        "df_final['tempo'] = df_final['tempo'].apply(lambda x: float(x))\n",
        "df_final['tempo'] = df_final['tempo'].astype('float64')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jee6UYVCICUY"
      },
      "source": [
        "Standardise column names.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGOm5s21H-II"
      },
      "outputs": [],
      "source": [
        "df_final.rename(columns = {\"Year\": \"year\", \"word counts\":\"word_counts\", \"unique word counts\": \"unique_word_counts\", \"Best_Ranking_Position\":\"best_ranking_position\",\n",
        "                           \"Days_Trending_US\": \"days_trending_US\",\"Days_Trending_ROW\": \"days_trending_ROW\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRJHiOIKI61C"
      },
      "outputs": [],
      "source": [
        "df_final.drop(columns = ['_merge'], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Re8ylR0JDGN"
      },
      "outputs": [],
      "source": [
        "df_final.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNIQZ04V_gUq"
      },
      "source": [
        "Clean-up to save memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrYwyYR0_fPq"
      },
      "outputs": [],
      "source": [
        "del df_song_features_1\n",
        "del df_song_features_2 \n",
        "del df_song_features_3 \n",
        "del df_song_features_4\n",
        "del df_song_features_5 \n",
        "del df_song_features_6 \n",
        "del df_song_features_7 \n",
        "del df_song_features_10 \n",
        "del df_tiktok_19 \n",
        "del df_tiktok_20\n",
        "del df_tiktok_21\n",
        "del feature_matches\n",
        "del features_10_rename_map\n",
        "del features_11_rename_map\n",
        "del features_12_rename_map\n",
        "del features_2_rename_map\n",
        "del features_1_rename_map\n",
        "del features_5_rename_map\n",
        "del features_3_rename_map\n",
        "del features_4_rename_map\n",
        "del features_7_rename_map\n",
        "del features_8_rename_map\n",
        "del features_9_rename_map\n",
        "del df_song_features_9\n",
        "del df_song_features_8\n",
        "del df_tiktok_full_merge\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVsMCjvWIE_i"
      },
      "source": [
        "# Final dataset EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk83wftcF_gU"
      },
      "source": [
        "## Word clouds\n",
        "\n",
        "Let's explore the most common words in songs per year. We decided to focus on frequent words that were unique for each year due to the overlap of very common words each year, e.g. yeah, know, like, love. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6hgXg5xGP4C"
      },
      "outputs": [],
      "source": [
        "df_lyrics_year = df_lyrics.merge(df_final[['title', 'year']], on='title', how = 'inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLJxZHwiG3xr"
      },
      "outputs": [],
      "source": [
        "df_lyrics_year = df_lyrics_year[['title', 'year', 'lyrics']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwXyE5FiF_gU"
      },
      "outputs": [],
      "source": [
        "lyrics_2017 = df_lyrics_year.loc[df_lyrics_year['year'] == 2017, 'lyrics'].tolist()\n",
        "top_tokens_list = [tokenize_content(x) for x in lyrics_2017]\n",
        "top_tokens_2017 = [t for sublist in top_tokens_list for t in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3yL7yLmF_gV"
      },
      "outputs": [],
      "source": [
        "lyrics_2018 = df_lyrics_year.loc[df_lyrics_year['year'] == 2018, 'lyrics'].tolist()\n",
        "top_tokens_list = [tokenize_content(x) for x in lyrics_2018]\n",
        "top_tokens_2018 = [t for sublist in top_tokens_list for t in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-985CkKKF_gV"
      },
      "outputs": [],
      "source": [
        "lyrics_2019 = df_lyrics_year.loc[df_lyrics_year['year'] == 2019, 'lyrics'].tolist()\n",
        "top_tokens_list = [tokenize_content(x) for x in lyrics_2019]\n",
        "top_tokens_2019 = [t for sublist in top_tokens_list for t in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbEiyOXrF_gV"
      },
      "outputs": [],
      "source": [
        "lyrics_2020 = df_lyrics_year.loc[df_lyrics_year['year'] == 2020, 'lyrics'].tolist()\n",
        "top_tokens_list = [tokenize_content(x) for x in lyrics_2020]\n",
        "top_tokens_2020 = [t for sublist in top_tokens_list for t in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PFHkvi_F_gV"
      },
      "outputs": [],
      "source": [
        "lyrics_2021 = df_lyrics_year.loc[df_lyrics_year['year'] == 2021, 'lyrics'].tolist()\n",
        "top_tokens_list = [tokenize_content(x) for x in lyrics_2021]\n",
        "top_tokens_2021 = [t for sublist in top_tokens_list for t in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpHYjdeGF_gV"
      },
      "outputs": [],
      "source": [
        "n = 1500  # adjust this for more or less words in the word cloud\n",
        "word_counts = Counter(top_tokens_2017)\n",
        "top_most_common = word_counts.most_common(n)\n",
        "print(\"2017:\", top_most_common)\n",
        "# and save only the words that are in english and not sensitive or profane\n",
        "words_2017 = set([x[0] for x in top_most_common if detect(x[0]) == 'en'])\n",
        "\n",
        "word_counts = Counter(top_tokens_2018)\n",
        "top_most_common = word_counts.most_common(n)\n",
        "print(\"2018:\", top_most_common)  \n",
        "words_2018 = set([x[0] for x in top_most_common if detect(x[0]) == 'en'])\n",
        "\n",
        "word_counts = Counter(top_tokens_2019)\n",
        "top_most_common = word_counts.most_common(n)\n",
        "print(\"2019:\", top_most_common) \n",
        "words_2019 = set([x[0] for x in top_most_common if detect(x[0]) == 'en'])\n",
        "\n",
        "word_counts = Counter(top_tokens_2020)\n",
        "top_most_common = word_counts.most_common(n)\n",
        "print(\"2020:\", top_most_common) \n",
        "words_2020 = set([x[0] for x in top_most_common if detect(x[0]) == 'en'])\n",
        "\n",
        "word_counts = Counter(top_tokens_2021)\n",
        "top_most_common = word_counts.most_common(n)\n",
        "print(\"2021:\", top_most_common)\n",
        "words_2021 = set([x[0] for x in top_most_common if detect(x[0]) == 'en'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7hGSytzE6Hd"
      },
      "source": [
        "Words like \"like\" and \"yeah\" are popular in every year. Are there any words that are uniquely frequent in each year?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHXRregHFBA6"
      },
      "outputs": [],
      "source": [
        "unique_to_2017 = words_2017 - words_2021 - words_2020 - words_2019 - words_2018 - set(sensitive_words)\n",
        "unique_to_2018 = words_2018 - words_2021 - words_2020 - words_2019 - words_2017 - set(sensitive_words)\n",
        "unique_to_2019 = words_2019 - words_2021 - words_2020 - words_2018 - words_2017 - set(sensitive_words)\n",
        "unique_to_2020 = words_2020 - words_2021 - words_2019 - words_2018 - words_2017 - set(sensitive_words)\n",
        "unique_to_2021 = words_2021 - words_2020 - words_2019 - words_2018 - words_2017 - set(sensitive_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiZNJe42KdhU"
      },
      "outputs": [],
      "source": [
        "# create a list of top tokens for each year\n",
        "top_tokens_list = [unique_to_2017, unique_to_2018, unique_to_2019, unique_to_2020, unique_to_2021]\n",
        "\n",
        "# create a grid of subplots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 6))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# loop through each year and create a word cloud subplot\n",
        "for i in range(len(top_tokens_list)):\n",
        "    wordcloud_frequencies = Counter(top_tokens_list[i])\n",
        "    top_tokens_wordcloud = WordCloud(background_color='white').generate_from_frequencies(wordcloud_frequencies)\n",
        "    ax = axes[i]\n",
        "    ax.imshow(top_tokens_wordcloud)\n",
        "    ax.set_title(\"Uniquely Frequently Used Words in \" + str(i + 2017))\n",
        "    ax.axis('off')\n",
        "\n",
        "# make the last subplot invisible and remove its axes\n",
        "axes[-1].set_visible(True)\n",
        "axes[-1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determining Ranking Classes"
      ],
      "metadata": {
        "id": "yws-g2YAYpn8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIF9tdzCna9H"
      },
      "source": [
        "To determine how to classify the data, we look at the distribution of ranks in the dataset. This is necessary because if we attempt to build a classifier based on training data that is imbalanced, with a significant majority of training instances in one class and very few in another, then we won't arrive at an accurate model. We can visualize the rank distribution in order to get a sense of where to draw the lines between classes so as to have balanced data.\n",
        "\n",
        "We would like to predict the following categories:\n",
        "\n",
        "*   top 5 hit\n",
        "*   top 50 \n",
        "*   top 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzzMXQrTnLbo"
      },
      "outputs": [],
      "source": [
        "df_final_position =  df_final.groupby(\"rank\")['title'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mz1HIhjnM_H"
      },
      "outputs": [],
      "source": [
        "print(df_final_position)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTxjuGJjnOss"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plot the data\n",
        "plt.plot(df_final_position)\n",
        "\n",
        "# set the plot title and axis labels\n",
        "plt.title('rank vs count')\n",
        "plt.xlabel('rank')\n",
        "plt.ylabel('count')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxlpAgxanRh7"
      },
      "outputs": [],
      "source": [
        "rows_1 = len(df_final[(df_final['rank'] == 1)])\n",
        "rows_top_10 = len(df_final[(df_final['rank'] <= 10)& (df_final['rank'] > 1)])\n",
        "rows_top_25 = len(df_final[(df_final['rank'] <= 25)& (df_final['rank'] > 10)])\n",
        "rows_top_50 = len(df_final[(df_final['rank'] <= 50)& (df_final['rank'] >25)])\n",
        "rows_top_100 = len(df_final[(df_final['rank'] <= 100)& (df_final['rank'] > 41)])\n",
        "rows_top_200 = len(df_final[(df_final['rank'] <= 200)& (df_final['rank']> 100)])\n",
        "rows_zero = len(df_final[(df_final['rank'] == 0)])\n",
        "rows_over_200 = len(df_final[(df_final['rank'] >200)])\n",
        "rows_total = len(df_final)\n",
        "\n",
        "print(\"rows 1:\", rows_1, int((rows_1/rows_total)*100), \"%\")\n",
        "print(\"rows top 10:\", rows_top_10, int((rows_top_10/rows_total)*100),\"%\")\n",
        "print(\"rows top 25:\", rows_top_25, int((rows_top_25/rows_total)*100),\"%\")\n",
        "print(\"rows top 50:\", rows_top_50, int((rows_top_50/rows_total)*100), \"%\")\n",
        "print(\"rows top 100:\", rows_top_100, int((rows_top_100/rows_total)*100), \"%\")\n",
        "print(\"rows top 200:\", rows_top_200, int((rows_top_200/rows_total)*100), \"%\")\n",
        "\n",
        "#print(\"rows 0:\", rows_zero, int((rows_zero/rows_total)*100))\n",
        "#print(\"rows over 200:\", rows_over_200, int((rows_over_200/rows_total)*100))\n",
        "print(\"total rows:\", rows_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q1Dz4O1Gz2K"
      },
      "source": [
        "We add the rank group to the final dataframe so that we have our classifications in place. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final['rank_group'] = pd.cut(df_final['rank'], bins=[0, 5, 50, 200], labels=[5, 50, 200])\n",
        "df_final.head()"
      ],
      "metadata": {
        "id": "GDE8QYbMFZWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoTSgIWSImZM"
      },
      "outputs": [],
      "source": [
        "# group and plot the data again\n",
        "df_final_rank_groups = df_final.groupby(\"rank_group\")['title'].count()\n",
        "\n",
        "# plot the data\n",
        "df_final_rank_groups.plot(x='rank_group', y='title', kind='bar')\n",
        "\n",
        "# set the plot title and axis labels\n",
        "plt.title('rank vs count')\n",
        "plt.xlabel('rank')\n",
        "plt.ylabel('count')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxRyCZdNKZ-3"
      },
      "source": [
        "We can see that there is class imbalance that will need to be addressed at the modelling stage. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# review the columns to see if we have outliers - looking for unusual min/max/mean values\n",
        "df_final_rank_review = df_final.groupby(by=['rank_group']).agg({'duration_ms':['mean', 'min', 'max'],'sentiment_gr':['mean', 'min', 'max'],'word_counts':['mean', 'min', 'max'],'unique_word_counts':['mean', 'min', 'max'],'best_ranking_position':['mean', 'min', 'max'],'days_trending_ROW':['mean', 'min', 'max'],'days_trending_US':['mean', 'min', 'max']})\n",
        "\n",
        "print(df_final_rank_review)"
      ],
      "metadata": {
        "id": "JOAOAYGjFkN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was repeated for different columns and we did not find clear outliers that should be removed. "
      ],
      "metadata": {
        "id": "xQ9wrOfQEEZk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww5h3CQD_ukW"
      },
      "source": [
        "## Visualizing Trends Over Time\n",
        "\n",
        "Before modeling and making predictions about what features contribute to a song's success, lets do some explorative data analysis to understand the data better. We'll explore and visualize the answers to questions such as\n",
        "\n",
        "- Have tastes in music been changing over time?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVRHwhxxgnb1"
      },
      "outputs": [],
      "source": [
        "# Lets get the average audio feature in every country in every year\n",
        "average_features = df_final[['year', 'danceability', 'energy', 'speechiness', 'acousticness', 'liveness', 'valence', 'tempo', 'duration_ms']].groupby(by=['year'], as_index=False).mean().round(3)\n",
        "\n",
        "# define the size of each subplot\n",
        "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(12, 6))\n",
        "\n",
        "# iterate through each feature and plot it in a subplot\n",
        "for i, feature in enumerate(['danceability', 'energy', 'speechiness', 'acousticness', 'liveness', 'valence', 'tempo', 'duration_ms']):\n",
        "    ax = axes[i//4, i%4]\n",
        "    average_features.plot(x='year', y=feature, ax=ax)\n",
        "    ax.set_title(feature.capitalize())\n",
        "    ax.set_xlabel('Year')\n",
        "    ax.set_ylabel('Score')\n",
        "    \n",
        "    if feature in ['danceability', 'energy', 'speechiness', 'acousticness', 'liveness', 'valence']:\n",
        "        ax.set_ylim(0, 1)\n",
        "    elif feature == 'tempo':\n",
        "        ax.set_ylim(110, 130)\n",
        "        ax.set_ylabel('Tempo (BPM)')\n",
        "    elif feature == 'duration_ms':\n",
        "        ax.set_ylim(190000, 240000)\n",
        "        ax.set_ylabel('Duration (ms)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn_MhdoVKCUu"
      },
      "source": [
        "The most significant change, which seems true from personal experience, is that average song duration in the charts got shorter from 2017 to 2021, from roughly 3 min 40 seconds to roughly 3 min 25 seconds. We can also see that there is a slight increase in Tempo, Valence, Acousticness and Danceability in the same period. It is worth noting that the averages calculated above include multiple entries per song if it charted in multiple countries. The effect is that the more popular a song is worldwide, the more heavily its features are weighted in the graphs above.\n",
        "\n",
        "Lets do the same with some of the lyric data. What about sentiment, has that changed through time? The amount of words used in songs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IncfAXL8cDL"
      },
      "outputs": [],
      "source": [
        "sentiment_by_year = df_final[['year', 'sentiment', 'word_counts', 'unique_word_counts']].groupby(by=['year'], as_index=False).mean()\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,3))\n",
        "\n",
        "# iterate through each feature and plot it in a subplot\n",
        "for i, feature in enumerate(['sentiment', 'word_counts', 'unique_word_counts']):\n",
        "  ax = axes[i]\n",
        "  sentiment_by_year.plot(x='year', y=feature, ax=ax)\n",
        "  plt.xlabel('Year')\n",
        "  ax.set_ylabel(feature)\n",
        "  plt.xticks(sentiment_by_year['year'])\n",
        "  if i == 1:\n",
        "    ax.set_title('Average Sentiment, Word Count, and Unique Word Count by Year')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*JOE* - sentiment score seems wrong here - the scale should be -1 -> +1."
      ],
      "metadata": {
        "id": "Fe5NWCVqEyaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interestingly, while the mean sentiment is positive, it became slighly less so through 2020 (perhaps correlating with Covid?), with a slight rebound in 2021. Another interesting trend is that word count seems to have had an inverse trend to the sentiment."
      ],
      "metadata": {
        "id": "CAA9vxzMu0T-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpTT6V1I0_qq"
      },
      "source": [
        "## Correlation between features "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkhWR0mH4C-A"
      },
      "source": [
        "Drop colums that won't be used for analysis and separate out rank as it will be the predicted dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk2e_oAXVp2F"
      },
      "outputs": [],
      "source": [
        "df_features_matrix = df_final[df_final['year'] > 2017].drop(['artist','sentiment','title', 'rank_group', 'year'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-s735JzUJ5u"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df_features_matrix, x_vars=['rank'], height=3, aspect=1.5, plot_kws={'s': 10})\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some interesting insights from the scatter matrix:\n",
        "- The tiktok popularity of a song corresponds quite well with its rank on Spotfify. \n",
        "- artists with highly ranks songs have spent longer periods of time trending.\n",
        "- highly ranks songs seem to be longer in duration. \n",
        "\n",
        "TODO COME BACK TO THIS: We can see that best_ranking_position isn't a good feature to use in our analysis because it isn't excluding the rank obtained by the song being analysed itself so this should be removed from the final analysis. "
      ],
      "metadata": {
        "id": "etTC95cgFJiQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2eie7oQEuVr"
      },
      "outputs": [],
      "source": [
        "# invert ranking since a low ranking in terms of its integer representation is actually a high ranking semantically\n",
        "df_features_matrix[['rank']] = df_features_matrix[['rank']].apply(lambda x: 201-x)\n",
        "df_features_matrix[['best_ranking_position']] = df_features_matrix[['best_ranking_position']].apply(lambda x: 201-x)\n",
        "corr_matrix = df_features_matrix.corr()\n",
        "mask = np.zeros_like(corr_matrix, dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# create a heatmap plot of the correlation matrix\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, fmt='.2f',vmin=-1, vmax=1, mask=mask)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation matrix has some interesting insights! But we have to be careful about how we interpret it, since a \"High\" rank is actually a low number, and a \"Low\" rank is actually a high number.\n",
        "\n",
        "Aside from some obvious multicollinearities which must be addressed before modelling, we can see that rank is actually not very correlated with any one feature. The largest correlation in magnitude is with tiktop popularity, at -0.07. This means that as a song becomes more popular on TikTok, its rank decreases, i.e. it also becomes more popular on Spotify. This answers one of our initial questions, espeically when compare that correlation with the correlations for \"days_trending_US\" and \"days_trending_ROW\". Both of these metrics measure how popular an artist was - their promenence - in the year of the release. We can see that being a prominent artist with ranking, but not as much as TikTok popularity does, which indicates that perhaps TikTok virality is becoming a bigger driver of song success than fame.\n",
        "\n",
        "Other interesting insights are that speechiness is much more highly correlated with trending in the US than in the rest of the world, indicating that perhaps Rap music is more popular in the US. Also, perhaps surprisingly is the insight that duration is negatively correlated with rank, meaning that as a song gets shorter, it is more likely to rank poorly (although this correlation is only 0.04). Also, out of all the audio features, danceability is the only one that correlates with better rankings.\n",
        "\n",
        "With recards to colinearity, we can see that word counts and unique word counts are highly correlated and days trending in the US and days trending in the ROW are highly correlated so we will remove one of each of these features from the final analysis. "
      ],
      "metadata": {
        "id": "KxHX65czT62S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGkseLEvE4kb"
      },
      "outputs": [],
      "source": [
        "del df_features_matrix\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ4utGyDG1aM"
      },
      "outputs": [],
      "source": [
        "del corr_matrix\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling: K-Means Clustering "
      ],
      "metadata": {
        "id": "W8GeSQBkL2uO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o0vTv0BRuqa"
      },
      "source": [
        "## Are There Natural Groupings of Popular Styles of Music?\n",
        "\n",
        "Subjectively, we have experienced that there are different styles of popular music. Dance music, acoustic singer-songwriter, electronic, etc., may be among the different types of charting popular music. Is it possible to identify natural clusters of songs in the charts based on audio feature or lyric sentiment?\n",
        "\n",
        "We'll experiment with K-Means Clustering to see if any natural groupings emerge.\n",
        "\n",
        "First, let's decide on which features are relevant to the k-means clustering. For example, song title, artist, and year are not relevant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDQ5tHymiu72"
      },
      "outputs": [],
      "source": [
        "possibly_relevant_features = ['duration_ms', 'tempo', 'energy', 'danceability', 'liveness', 'valence', 'acousticness', 'speechiness', 'sentiment', 'word_counts', 'unique_word_counts']\n",
        "\n",
        "# create an empty list to store the sum of squared distances for each k\n",
        "ssd = []\n",
        "k_range = range(1, len(possibly_relevant_features))\n",
        "\n",
        "# fit KMeans for each k value and record the sum of squared distances\n",
        "for k in k_range:\n",
        "    km = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "    km.fit(df_final[possibly_relevant_features])\n",
        "    ssd.append(km.inertia_)\n",
        "# plot the elbow curve\n",
        "plt.plot(k_range, [x/max(ssd) for x in ssd], 'bx-')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Normalized Sum of Squared Distances')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzrfTUCBlyrz"
      },
      "source": [
        "It looks like k=6 clusters contains 90 percent of the information of natural groupings, at which point there are diminishing returns for increasing k. Is it possible to narrow down the subset of features that contain most of the information of natural groupings?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yJDfZQeoGkG"
      },
      "outputs": [],
      "source": [
        "feature_cols = ['duration_ms', 'tempo', 'energy', 'danceability', 'liveness', 'valence', 'acousticness', 'speechiness', 'sentiment','word_counts', 'unique_word_counts']\n",
        "n_features = 7\n",
        "\n",
        "ssd_results = [math.inf] * (n_features-1)\n",
        "subsets = [0] * (n_features-1)\n",
        "\n",
        "for r in range(2, n_features+1):\n",
        "    for subset in itertools.combinations(feature_cols, r):  # iterate through all combinations of features\n",
        "        subset = list(subset)\n",
        "        # since prev r-1 feature subset explained the most of the clustering, next feature subset should include those features. speeds up run time\n",
        "        if r > 2 and not set(subsets[r-3]).issubset(set(subset)):\n",
        "          continue\n",
        "        km = KMeans(n_clusters=6, init='k-means++', random_state=42)\n",
        "        km.fit(df_final[subset])\n",
        "        # below, \"/10**(r-1)\" is a scaling factor so we can compare the sum mean squared distances of the k means clustering over different dimensions\n",
        "        # We divide the sum of distances in two dimensions by 10^0. Divide the sum of distances in three dimensions by 10^1. In four dimensions, 10^2...\n",
        "        if km.inertia_ / 10**(r-2) < ssd_results[r-2]:\n",
        "          ssd_results[r-2] = km.inertia_ / 10**(r-2)  \n",
        "          subsets[r-2] = subset\n",
        "\n",
        "# plot the elbow curve for each subset of features\n",
        "plt.plot(range(2, n_features+1), [x/max(ssd_results) for x in ssd_results], 'bx-')\n",
        "plt.xlabel('Number of features')\n",
        "plt.ylabel('Normalized Sum of Squared Distances')\n",
        "plt.title('Elbow Method for Optimal Number of Features')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp2QVeyR2mCI"
      },
      "source": [
        "Clearly four or five features is the point at which we start seeing diminishing returns. Which features are those that contain the most information with respect to natural clustering? The five most relevant features for explaining the natural gourpings are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13d8L-_jwy2q"
      },
      "outputs": [],
      "source": [
        "k_means_feature_subset = subsets[3]\n",
        "subsets[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6VLoWfg3_BJ"
      },
      "source": [
        "What does this mean? We can notice that, for example, 'sentiment' is not included in the list of features that contain the most information of the natural groupings. This means that while there is variation in sentiment among the songs, two songs with opposite sentiment but with the same energy level are more likely to belong to the same natural group than two songs with opposite energy levels but the same sentiment. Making this more relatable, this is to say that songs that are high energy, danceable, live, acoustic and speechy are most likely to belong to the same group, regardless of whether they contain positive or negative sentiment, are short or long, contain many or few words, etc.\n",
        "\n",
        "It may be insightful in future modeling to have a cluster label column, which may end up being a strong predictor of success."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H53e8v7rWaI"
      },
      "outputs": [],
      "source": [
        "km = KMeans(n_clusters=6, init='k-means++', random_state=42)\n",
        "km.fit(df_final[subsets[3]])\n",
        "df_final['cluster'] = km.predict(df_final[subsets[3]])\n",
        "df_final[10986:10991][['title', 'artist','cluster']]  # look at random slice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_DQG_2CtIz9"
      },
      "source": [
        "Lets listen to some music and see if these clusters make sense subjectively!\n",
        "\n",
        "Cluster 5:\n",
        "\n",
        "[How Do I Live - LeAnn Rimes](https://youtu.be/1Olo8gzgpC4)\n",
        "\n",
        "[How Do We Make It - Jarryd James](https://youtu.be/PZY_y-OWw4Y)\n",
        "\n",
        "Cluster 1:\n",
        "\n",
        "[How Do You Do! - Roxette](https://youtu.be/nx2iLOvP0rM)\n",
        "\n",
        "[How Do You Feel? - The Maine](https://youtu.be/tku0H7YeMeA)\n",
        "\n",
        "Subjectively, these songs in these clusters sound very similar! Interestingly, the second \"How Do I Live\" song is the same song, but with slightly different instrumentation.\n",
        "\n",
        "[How Do I Live - Trisha Yearwood](https://youtu.be/Rcy6hNthQaA)\n",
        "\n",
        "What are the characteristics of each of the clusters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZWibsWzw9No"
      },
      "outputs": [],
      "source": [
        "df_cluster_means = df_final[subsets[3] + ['cluster']].groupby(by=['cluster']).mean()\n",
        "df_cluster_means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bdZ_PxaxgHt"
      },
      "source": [
        "Some insights that jump out right away are:\n",
        "- Cluster 0 contains the most live sounding songs\n",
        "- Cluster 1 contains the highest energy songs\n",
        "- Cluster 2 contains the most danceable songs\n",
        "- Cluster 3 contains acoustic high energy and danceable songs\n",
        "- Cluster 4 contains mellow acoustic songs\n",
        "- Cluster 5 contains more mellow, non-acoustic songs\n",
        "\n",
        "Let's create columns for each of these clusters and one-hot encode so they can be used in further machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdv3NJcJ0hbg"
      },
      "outputs": [],
      "source": [
        "clusters = ['genre_live', 'genre_high_energy_non_acoustic', 'genre_dance', 'genre_high_energy_dance', 'genre_mellow_acoustic', 'genre_mellow_non_acoustic']\n",
        "for i in range(6):\n",
        "  df_final[clusters[i]] = df_final['cluster'].apply(lambda x: 1 if x == i else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upK5lNDx2XKZ"
      },
      "source": [
        "And let's reduce the five dimensions that we calculated the k-means clustering over down to two via PCA to visualize the cluster distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMv3GSHI2kTK"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca_scores = pca.fit_transform(df_final[k_means_feature_subset])\n",
        "pca_df = pd.DataFrame({'Dimension 1': pca_scores[:,0], 'Dimension 2': pca_scores[:,1], 'cluster': df_final['cluster']})\n",
        "ax = sns.scatterplot(data=pca_df, x='Dimension 1', y='Dimension 2', hue='cluster', palette='Set1')\n",
        "hands, labs = ax.get_legend_handles_labels()\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Genre', handles= hands, labels=[clusters[i].replace('_', ' ').replace('genre ', '').capitalize() for i in range(6)])\n",
        "plt.title('PCA Visualization of Genre Distributions')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMoQuBgkAKjp"
      },
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2, perplexity=10, n_iter=250, n_iter_without_progress=50)  # Delete all args for clearer viz, longer run time\n",
        "tsne_scores = tsne.fit_transform(df_final[k_means_feature_subset])\n",
        "colors = sns.color_palette('Set1', n_colors=6)\n",
        "tsne_df = pd.DataFrame({'Dimension 1': tsne_scores[:,0], 'Dimension 2': tsne_scores[:,1], 'cluster': df_final['cluster']})\n",
        "ax = sns.scatterplot(data=tsne_df, x='Dimension 1', y='Dimension 2', hue='cluster', palette=colors)\n",
        "hands, labs = ax.get_legend_handles_labels()\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', handles=hands, labels=[clusters[i].replace('_', ' ').replace('genre ', '').capitalize() for i in range(6)], title='Genre')\n",
        "plt.title('t-SNE Visualization of Genre Distributions')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMo5m1rUEx7e"
      },
      "source": [
        "## Song recommender\n",
        "What metric can we use to recommend songs?\n",
        "\n",
        "We'll assume that if a listener likes one particular song, the listener will also like songs with similar features. To create a list of recommended songs, we select the ones that are the nearest (measued as the euclidean distance) to the provided song."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWILex7gEzj4"
      },
      "outputs": [],
      "source": [
        "#Get selected song's parameters (use title and artist for seletion)\n",
        "song = ('How Do I Live', 'LeAnn Rimes')\n",
        "liked_song_full_features = df_final[(df_final['title']==song[0]) & (df_final['artist']==song[1])]\n",
        "\n",
        "#Copy a subset of the features\n",
        "liked_song_pred_features = liked_song_full_features[['energy', 'danceability', 'liveness', 'acousticness', 'speechiness']].copy()\n",
        "\n",
        "#Prediction of in what cluster it should fit\n",
        "pred = km.predict(liked_song_pred_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYMCPPrU5-EF"
      },
      "outputs": [],
      "source": [
        "#Calculate euclidean distance and select the 5 closest to the entered song\n",
        "\n",
        "#Convert DF to Array\n",
        "df_test = df_final[['energy', 'danceability', 'liveness', 'acousticness', 'speechiness']].to_numpy()\n",
        "\n",
        "#Calculate euclidean distance for all songs to the selected song\n",
        "distances = pd.DataFrame(euclidean_distances(liked_song_pred_features.to_numpy().reshape(1, -1), df_test)).transpose()\n",
        "\n",
        "#Sort distances by descending order and pickt the top 6 (the first would be the selected song)\n",
        "distances.rename(columns={0:'Distance'}, inplace=True)\n",
        "recommendation = distances.sort_values(\"Distance\", ascending=True).head(6)\n",
        "\n",
        "#Join nearest six songs with df_final to get song details\n",
        "recommendation = recommendation.join(df_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bq7jXhME5OY"
      },
      "outputs": [],
      "source": [
        "# Project recomendation to selected fields\n",
        "recommendation[['Distance','title','artist','energy', 'danceability', 'liveness', 'acousticness', 'speechiness']]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that somebody likes [How Do I Live, by LeAnn Rimes](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjor9--38j-AhX-EVkFHRFXA3UQyCl6BAgcEAM&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DMUFasKZcH_c&usg=AOvVaw3T47vFZxGt7UTNW9MDxYl4), here are the recommended songs:\n",
        "\n",
        "\n",
        "[You Don't Do It For Me Anymore, by Demi Lovato\t](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjFn-Lo38j-AhV2D1kFHaysBTIQyCl6BAgVEAM&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DymJsTWxziKI&usg=AOvVaw21sF72KlWlzBi_bR6ldStP)\n",
        "\n",
        "[Girlie, by\tAlexandra Savior](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiSjqL938j-AhWVMVkFHa0sDA0QyCl6BAgYEAM&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D9a65ZcLM_1E&usg=AOvVaw1HPO8XfwBjsjgG1ak-X7Ms)\n",
        "\n",
        "[Мамонтёнок, by LSP](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj-7OiM4Mj-AhXiFFkFHfSRDKkQtwJ6BAgOEAI&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DJQXoVfW3z6E&usg=AOvVaw1qT4Kx--1gHjZ_pscOX3b2)\n",
        "\n",
        "[Always (Outro), by\tBryson Tiller](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiJ4bK74Mj-AhUXM1kFHQp3BoMQyCl6BAgVEAM&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dcx1fDepOJ20&usg=AOvVaw2fYNJVj-8dDecwbwPkjZ8L)\n",
        "\n",
        "[蒲公英的約定, by\tJay Chou\t](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiK6PPS4Mj-AhXsK1kFHTGaAU0QyCl6BAgVEAM&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DVitJnr3IySc&usg=AOvVaw23bTu8cyk16xJ1iGewIGJB)\n",
        "\n"
      ],
      "metadata": {
        "id": "UX_4sk6-jQXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extending the recommendation algorithm to a list of songs\n",
        "\n",
        "Now, let's try the same but with a list of songs instead of a single song. In this case, we calculate the mean of the features and then look for the Nearest Neighbours"
      ],
      "metadata": {
        "id": "CXNv6zhNdMb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a list of tuples (title, artist)\n",
        "song_list = [('How Do I Live', 'LeAnn Rimes'), ('Ciega, Sordomuda', 'Shakira'), ('When To Say When', 'Drake'), ('Roar', 'Katy Perry')]\n",
        "\n",
        "#Get selected song parameters (use title and artist for seletion)\n",
        "liked_song_pred_features_list = []\n",
        "for i in song_list:\n",
        "  temp = df_final[(df_final['title']==i[0]) & (df_final['artist']==i[1])]\n",
        "  temp = temp[['energy', 'danceability', 'liveness', 'acousticness', 'speechiness']]\n",
        "  liked_song_pred_features_list.append(temp)\n",
        "liked_song_pred_features_list = pd.concat(liked_song_pred_features_list)\n",
        "\n",
        "#Calculate a new vector with mean of features\n",
        "features_mean = pd.DataFrame(np.mean(liked_song_pred_features_list)).transpose()\n"
      ],
      "metadata": {
        "id": "C6LtcrCBdZNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In what cluster each of the songs fit individually?"
      ],
      "metadata": {
        "id": "-pOt4Cvfjn0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#iterate over liked songs to show to what cluster they belong to\n",
        "for index, row in liked_song_pred_features_list.iterrows():\n",
        "  cluster = km.predict(pd.DataFrame(row).transpose())\n",
        "  print(df_final.iloc[index]['title'], ', by ', df_final.iloc[index]['artist'], ' belongs to cluster: ', cluster[0])"
      ],
      "metadata": {
        "id": "IsWdf7B-dgLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the mean of the songs were a song, it'll be in Cluster 1: the highest energy songs."
      ],
      "metadata": {
        "id": "POIv9Itrjw9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction of in what cluster it should fit. We're using this data \n",
        "pred = km.predict(features_mean)\n",
        "print('Cluster for mean of liked songs: ', pred[0])"
      ],
      "metadata": {
        "id": "4FShCNyddkDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's look at the closest songs"
      ],
      "metadata": {
        "id": "ub8T_NuWj6HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate euclidean distance and select the 5 closest to the song\n",
        "df_test = df_final[['energy', 'danceability', 'liveness', 'acousticness', 'speechiness']].to_numpy()\n",
        "distances = pd.DataFrame(euclidean_distances(features_mean.to_numpy().reshape(1, -1), df_test)).transpose()\n",
        "distances.rename(columns={0:'Distance'}, inplace=True)\n",
        "recommendation = distances.sort_values(\"Distance\", ascending=True).head(6)\n",
        "recommendation = recommendation.join(df_final)"
      ],
      "metadata": {
        "id": "4o9KaGHEj9ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And these are the recommended ones"
      ],
      "metadata": {
        "id": "ZISAzAq_kBGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Recommendation\n",
        "recommendation[['Distance','title','artist','energy', 'danceability', 'liveness', 'acousticness', 'speechiness']]"
      ],
      "metadata": {
        "id": "Bf4V6Bcwj_8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#iterate over liked songs to show to what cluster they belong to\n",
        "for index, row in recommendation.iterrows():\n",
        "  cluster = km.predict(pd.DataFrame(row[['energy', 'danceability', 'liveness', 'acousticness', 'speechiness']]).transpose())\n",
        "  print(row['title'], ', by ',row['artist'], ' belongs to cluster: ', cluster[0])"
      ],
      "metadata": {
        "id": "SvqyVEuFkLvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All recommended songs belong to the same cluster a the mean of liked songs."
      ],
      "metadata": {
        "id": "wW8oKbP_kT0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the liked songs:\n",
        "\n",
        "[How Do I Live, by LeAnn Rimes\t](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjor9--38j-AhX-EVkFHRFXA3UQyCl6BAgcEAM&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DMUFasKZcH_c&usg=AOvVaw3T47vFZxGt7UTNW9MDxYl4)\n",
        "\n",
        "[Ciega, Sordomuda, by Shakira](https://www.youtube.com/watch?v=B3gbisdtJnA)\n",
        "\n",
        "[When To Say When, by Drake](https://www.youtube.com/watch?v=qTNFIQyWe8M)\n",
        "\n",
        "[Roar, by Katy Perry](https://www.youtube.com/watch?v=CevxZvSJLk8)\n",
        "\n",
        "\n",
        "Here are the recommended songs:\n",
        "\n",
        "[Too Bad, by IshDARR](https://www.youtube.com/watch?v=-heyCmkqwZM)\n",
        "\n",
        "[Reuben James, by\tKenny Rogers & The First Edition](https://www.youtube.com/watch?v=8LthFi2T4Hk)\n",
        "\n",
        "[Waiting On the World to Change, by\tJohn Mayer](https://www.youtube.com/watch?v=oBIxScJ5rlY)\n",
        "\n",
        "[Good Lovin', by\tBenjamin Ingrosso\t](https://www.youtube.com/watch?v=pwpmyvl2csc)\n",
        "\n",
        "[Sharingan, by Reckol\t](https://www.youtube.com/watch?v=MfOLZOcRA10)\n",
        "\n",
        "[Crime Pays, by\tFreddie Gibbs, Madlib\t](https://www.youtube.com/watch?v=u8R7fmLYgi4)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u56f4TjBkhP6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z9C53IKR0g6"
      },
      "source": [
        "# Modeling: Classifying hit songs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1rN31NKRmVE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import itertools\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlDzrnD7Aqs3"
      },
      "outputs": [],
      "source": [
        "del sensitive_words\n",
        "del df_lyrics\n",
        "del df_lyrics_high_counts\n",
        "del df_lyrics_negative\n",
        "del df_lyrics_ready\n",
        "del df_lyrics_positive\n",
        "del df_lyrics_zero\n",
        "del df_tiktok_matches_final\n",
        "del df_tiktok_tracks_only\n",
        "del df_features_artist_format_1\n",
        "del df_features_artist_format_2\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We dropped columns where there was stronger correlation between features."
      ],
      "metadata": {
        "id": "wrM_MP4kBEJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTyZeSsT3SvC"
      },
      "outputs": [],
      "source": [
        "# stores features dataframe into variable called \"features\"\n",
        "df_features = df_final.drop(columns = ['cluster', 'artist', 'sentiment', 'sentiment_gr', 'unique_word_counts','days_trending_ROW', 'rank', 'rank_group'])#, 'year'])#,'best_ranking_position'])\n",
        "\n",
        "# set title as the index\n",
        "df_features.set_index('title', inplace=True)\n",
        "\n",
        "# store the regression target variable\n",
        "rank = df_final['rank_group']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3031qbf-qUKc"
      },
      "outputs": [],
      "source": [
        "df_features.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJfeTmABNSLU"
      },
      "source": [
        "## Address class imbalances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U1MR5EXNdh7"
      },
      "source": [
        "During EDA we noted that our classes were not balanced with the topp 100 category having more datapoint. We will use SMOTE to correct class imbalances by oversampling classes that have less rows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auR3kQgZNRY2"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "\n",
        "# summarize class distribution\n",
        "counter = Counter(rank)\n",
        "print(\"Before SMOTE:\",counter)\n",
        "# transform the dataset\n",
        "oversample = SMOTE(random_state=42)\n",
        "df_features, rank = oversample.fit_resample(df_features, rank)\n",
        "# summarize the new class distribution\n",
        "counter = Counter(rank)\n",
        "print(\"After SMOTE:\", counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After SMOTE, we have 19,488 rows for each class. "
      ],
      "metadata": {
        "id": "K7U_pMqbaKo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rank = rank[df_features['year'] > 2017].drop(columns='year')\n",
        "df_features = df_features[df_features['year'] > 2017].drop(columns='year')"
      ],
      "metadata": {
        "id": "Q_BsIRCVwD94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features.shape[0]"
      ],
      "metadata": {
        "id": "K8_dImy8xstc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvWA7Sv_4vUY"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Since we are working on a multi-class classification problem, we decided to test out logistic regression to start. \n",
        "\n",
        "We tried using two different multi_class methods. 'ovr' fits a binary problem for each label and ‘multinomial’ is the multinomial loss fit across the entire probability distribution.\n",
        "\n",
        "The solver chosen was 'lbfgs' to account for the fact that the features have not yet been scaled and compatibility with the multi_class analysis. \n",
        "\n",
        "The max iterations value was set to 1000 because the model would not converge at the default value of 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C11HBmoC4ZLC"
      },
      "source": [
        "Split data into test and training set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k49sDvtV4TQ4"
      },
      "outputs": [],
      "source": [
        "# Assign appropriate value to seed and conduct 80/20 train-test split with random_state = seed\n",
        "seed = 42\n",
        "x = df_features\n",
        "y = rank\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Data Size: features x:{},  label y:{}\".format(x_train.shape, y_train.shape))\n",
        "print(\"Testing Data Size: features x:{},  label y:{}\".format(x_test.shape, y_test.shape))"
      ],
      "metadata": {
        "id": "LQ-a5JOicS6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiDKolBw4sLT"
      },
      "outputs": [],
      "source": [
        "# Initialize model with default parameters and fit it on the training set\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "clf = LogisticRegression(multi_class = 'ovr', max_iter=10000, solver = 'lbfgs')\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "# Use the model to predict on the test set and save these predictions as `y_pred`\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "# Find the accuracy and store the value in `log_acc`\n",
        "train_accuracy = clf.score(x_train, y_train)\n",
        "log_accuracy = accuracy_score(y_pred,y_test)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Test accuracy: {log_accuracy}\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to look beyond the accuracy score as this may not make it clear when there are issues classifying certain classes. In this model we can see that the under 10s class had an f1-score of 0.7 but the model didn't perform as well for the other classes. "
      ],
      "metadata": {
        "id": "aWQR2IrPnbtr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7aafrQwKudX"
      },
      "source": [
        "Next we tried setting 'multi_class' = 'multinomial'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzV7SJeRF1Nn"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=10000, random_state=42)\n",
        "\n",
        "# fit the model on the training data\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# predict the labels for the test data\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "# calculate the accuracy score\n",
        "train_accuracy = clf.score(x_train, y_train)\n",
        "log_multi_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Accuracy: {log_multi_accuracy}\")\n",
        "\n",
        "# print the classification report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing to 'multi-nomial' did not improve performance much, we see the model is still best at predicting the top ten class. "
      ],
      "metadata": {
        "id": "Jn6waDArn8Np"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxXpifNK4_ks"
      },
      "source": [
        "### PCA to reduce dimensionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxVGanpY5svS"
      },
      "source": [
        "PCA transforms the features into a new set of uncorrelated features called principal components.\n",
        "\n",
        "We wanted to see if it can reduce the complexity of our dataset and improve performance of the logistic regression model. The first step is to scale the data as regularisation is not scale invariant. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yMLh23y5Dxs"
      },
      "outputs": [],
      "source": [
        "# Address scale-invariance\n",
        "sc = StandardScaler()\n",
        "x_scaled_train = sc.fit_transform(x_train)\n",
        "x_scaled_test = sc.transform(x_test)\n",
        "\n",
        "# Instantiate and Fit PCA\n",
        "pca = PCA()\n",
        "pca_fit = pca.fit(x_scaled_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oteBPsaz5ZVq"
      },
      "outputs": [],
      "source": [
        "# Get the explained variance for each component\n",
        "explained_variance_ratios = pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYMjR7te5nPw"
      },
      "outputs": [],
      "source": [
        "# Save the CUMULATIVE explained variance ratios into variable called \"cum_evr\"\n",
        "cum_evr = np.cumsum(explained_variance_ratios)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG0USAvZ5oM_"
      },
      "outputs": [],
      "source": [
        "# find optimal num components to use (n) by plotting explained variance ratio\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(1,len(cum_evr)+1), cum_evr,label='Cumulative explained variance')\n",
        "plt.axhline(y=0.8, c=\"black\", linewidth=1, zorder=0, label = '80% threshold')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.legend(loc='best')\n",
        "plt.xticks(np.arange(1, 19, 1))\n",
        "plt.title('explained variance ratio vs principal component index')   \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwliVFrz51Tw"
      },
      "source": [
        "Using our results from the PCA visualization above, we decide to keep 11 components because it explains at least 80% of total variance in the dataset. We now re-fit and transform the PCA on the training set with these components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIjlWggn5zo9"
      },
      "outputs": [],
      "source": [
        "# 1. Refit and transform on training with parameter n (as deduced from the last step) \n",
        "pca = PCA(n_components=11)\n",
        "x_train_pca = pca.fit_transform(x_scaled_train)\n",
        "\n",
        "# 2. Transform on Testing Set and store it as `x_test_pca`\n",
        "x_test_pca = pca.transform(x_scaled_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Mh_hNaV6LUn"
      },
      "outputs": [],
      "source": [
        "# Now initialise `log_reg_pca` model with default parameters and fit it on the PCA transformed training set\n",
        "log_reg_pca = LogisticRegression(multi_class = 'ovr', max_iter=10000, random_state=42)\n",
        "log_reg_pca.fit(x_train_pca,y_train)\n",
        "\n",
        "# Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`\n",
        "y_pred = log_reg_pca.predict(x_test_pca)\n",
        "\n",
        "#  Find the accuracy and store the value in `test_accuracy`\n",
        "train_accuracy = log_reg_pca.score(x_train_pca, y_train)\n",
        "log_pca_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Test accuracy: {log_pca_accuracy}\")\n",
        "# print the classification report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our test and train accuracy was not improved by using PCA. We can see that the ability to predict the top 200 class did improve but the top 100 class is still not performing well. There are different reasons why PCA may not be helping, e.g. non-linear relationships between features and information loss."
      ],
      "metadata": {
        "id": "MGG6If8aonWl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpoXYa8hGf6A"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "log_mult_pca = LogisticRegression(multi_class='multinomial', max_iter=10000, random_state=42)\n",
        "\n",
        "# fit the model on the training data\n",
        "log_mult_pca.fit(x_train_pca, y_train)\n",
        "\n",
        "# predict the labels for the test data\n",
        "y_pred = log_mult_pca.predict(x_test_pca)\n",
        "\n",
        "# calculate the accuracy score\n",
        "log_mult_pca_aaccuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {log_mult_pca_aaccuracy}\")\n",
        "\n",
        "# print the classification report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We double-checked this using the 'multinomial' multi-class the performance didn't change significantly. "
      ],
      "metadata": {
        "id": "7kJM-e2tpMZ7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDYolfVT774V"
      },
      "source": [
        "### Introducing Regularisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhkP9YOZ8Aaq"
      },
      "source": [
        "We'll try a number of regularisation techniques on the data to see if the performance of the model can be improved. The default value for C was used to get a general idea of the performance of models initially. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQppehT38Jg7"
      },
      "source": [
        "#### Ridge regularisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gbdq0Fs8Hsg"
      },
      "outputs": [],
      "source": [
        "# fit the logistic regression model with regularization\n",
        "lr = LogisticRegression(penalty='l2', C=1.0, solver='saga' ,multi_class = 'ovr')\n",
        "lr.fit(x_train_pca, y_train)\n",
        "\n",
        "# predict on the test set\n",
        "y_pred = lr.predict(x_test_pca)\n",
        "\n",
        "#  Find the accuracy and store the value in `ridge_test_accuracy`\n",
        "train_accuracy = lr.score(x_train_pca, y_train)\n",
        "ridge_test_accuracy = accuracy_score(y_pred,y_test)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Train accuracy: {ridge_test_accuracy}\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA4Q3UGaBUUn"
      },
      "source": [
        "#### Lasso regularisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgRyNdsR8cKa"
      },
      "source": [
        "Since we didn't see evidence of multicollinearity in the data once 'word_counts' and 'trending_days_ROW' were removed we'll try to run lasso regression. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpLo1Y8P8aQy"
      },
      "outputs": [],
      "source": [
        "# fit the logistic regression model with regularization\n",
        "lr = LogisticRegression(penalty='l1', C=1.0, solver='saga',multi_class = 'ovr')\n",
        "lr.fit(x_train_pca, y_train)\n",
        "\n",
        "# predict on the test set\n",
        "y_pred = lr.predict(x_test_pca)\n",
        "\n",
        "#  Find the accuracy and store the value in `ridge_test_accuracy`\n",
        "lasso_test_accuracy = accuracy_score(y_pred,y_test)\n",
        "print(lasso_test_accuracy)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LQ7n3TUBfHu"
      },
      "source": [
        "#### Elastic net regularisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7dmrvL_BmRA"
      },
      "outputs": [],
      "source": [
        "# fit the logistic regression model with regularization\n",
        "en = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0,multi_class = 'ovr')\n",
        "en.fit(x_train_pca, y_train)\n",
        "\n",
        "# predict on the test set\n",
        "y_pred = en.predict(x_test_pca)\n",
        "\n",
        "#  Find the accuracy and store the value in `ridge_test_accuracy`\n",
        "en_test_accuracy = accuracy_score(y_pred,y_test)\n",
        "train_accuracy = lr.score(x_train_pca, y_train)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Train accuracy: {en_test_accuracy}\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularisation techniques did not seem to impact performance significantly. We decided to try some other modelling techniques rather than tuning parameters as another model may give a better chance of significantly increasing the model performance."
      ],
      "metadata": {
        "id": "ZWvJRM2AplSd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QtzQ8mVCZc4"
      },
      "source": [
        "## Decision trees\n",
        "\n",
        "We decided to use tree based models to see if we could improve on the results observed with linear regression. A DecisionTree was trialed first and an immediate improvement in the accuracy was observed. This model is scare invarient so the original features data used. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb5P3Gv0_2yD"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Create an instance of the Decision Tree classifier\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier on the training set\n",
        "dt.fit(x_train, y_train)\n",
        "\n",
        "# Use the trained classifier to predict the classifications of the testing set\n",
        "y_pred = dt.predict(x_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "dt_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "train_accuracy = dt.score(x_train, y_train)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print('Test accuracy:', dt_accuracy)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The overall model accuracy improved and the predictions across classes improved. This is the best model so far but we can see that the training accuracy is higher than the test accuracy with suggests that the model may be overfitting the training data. \n",
        "\n",
        "We decided to scale this to ensemble trees with a Random Forest classifer to try to further improve the model. The accuracy did increase when we switched to using ensemble trees. "
      ],
      "metadata": {
        "id": "mGHwY7lTp8B7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsDmFaf7CgJd"
      },
      "source": [
        "## Random Forest classifer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First test the Random Forest Classifier on the data set"
      ],
      "metadata": {
        "id": "9pueUng1w00P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTL1SQ4AASMM"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Create an instance of the Decision Tree classifier\n",
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rfc.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = rfc.predict(x_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "rf_accuracy = accuracy_score(y_test, y_pred)\n",
        "train_accuracy = rfc.score(x_train, y_train)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print('Test accuracy:', rf_accuracy)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next test the Random Forest Classifier on the data set that has been transformed accoring to PCA"
      ],
      "metadata": {
        "id": "-X7Cu4wBw8VI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVdhhhQuV8iC"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the Decision Tree classifier\n",
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rfc.fit(x_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = rfc.predict(x_test_pca)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "rf_pca_accuracy = accuracy_score(y_test, y_pred)\n",
        "train_accuracy = rfc.score(x_train_pca, y_train)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(\"Test accuracy:\", rf_pca_accuracy)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNWaeMH4NPjN"
      },
      "source": [
        "### Hyperparameter tuning\n",
        "\n",
        "Since we saw good performance with the RandomForest classifer relative to the other models so far, we decided to try tune the hyperparameters for this model to see if we could further improve performance. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "depth_list= range(2, 26, 2)\n",
        "model_list = []\n",
        "train_score_list = []\n",
        "test_score_list = []\n",
        "\n",
        "for depth in depth_list: \n",
        "    # split the training data and test data for each round\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df_features, rank, test_size= 0.2, random_state=42)\n",
        "    # initialize the random forest classifiers\n",
        "    clf = RandomForestClassifier(max_depth= depth);\n",
        "    clf.fit(X_train, y_train);\n",
        "    model_list.append(clf);\n",
        "    train_score = clf.score(X_train, y_train);\n",
        "    train_score_list.append(train_score);\n",
        "    test_score = clf.score(X_test, y_test);\n",
        "    test_score_list.append(test_score);\n",
        "    depth = format(depth, '.1f');\n",
        "    train_score = format(train_score, '.13f');\n",
        "    test_score = format(test_score, '.13f');\n",
        "    print(\"Random forest tree max_depth: {} || train_accuracy: {} || test_accuracy: {}\".format(depth, train_score, test_score));"
      ],
      "metadata": {
        "id": "f9Vy7fZTsh-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-tj2mDQgIFe"
      },
      "outputs": [],
      "source": [
        "plt.plot(depth_list, train_score_list, '-', label = 'train_accuracy')\n",
        "plt.plot(depth_list, test_score_list, '--', label = 'test_accuracy')\n",
        "plt.title('{} Model Accuracy with different {} params'.format('Random Forest', 'max_depth'))\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc= 0, prop={'size': 10})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We choose a max depth of 8 so that the model is not overfitting the training data. "
      ],
      "metadata": {
        "id": "83xutNq13krB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_features, rank, test_size= 0.2, random_state=42)\n",
        "# Create an instance of the Decision Tree classifier\n",
        "best_model = RandomForestClassifier(max_depth=8)\n",
        "\n",
        "# Fit the model to the training data\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "bm_accuracy = accuracy_score(y_test, y_pred)\n",
        "train_accuracy = best_model.score(X_train, y_train)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print('Test accuracy:', bm_accuracy)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "31u9kcHkrJEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the test accuracy and train accuracy are closer and there is a good balance across all classes for the F1 score. This is the best model so far. \n",
        "\n",
        "We plot the predicted vs real values for test and train for a subset of data to visually review the outputs. We can see they are similar. "
      ],
      "metadata": {
        "id": "PDz8HeIz3mux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = best_model.predict(X_test)\n",
        "num_data_points = 200\n",
        "x_axis_label = 'data point'\n",
        "\n",
        "#def compare_predicted_and_true_value(num = 200, x_label = 'x', y_test= [], predicted_y_test= []):\n",
        "l = range(250)\n",
        "fig, (ax1, ax2) = plt.subplots(2, sharex=False, figsize=(9, 6))\n",
        "ax1.plot(l, y_test[0:250],'r')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('true y value')\n",
        "ax1.set_title('True y value in test dataset (for first {} data points)'.format(250))\n",
        "ax2.plot(l, y_predicted[0:250], 'b')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('predicted y value')\n",
        "ax2.set_title('Predicted y value in test dataset (for first {} data points)'.format(250))\n",
        "fig.tight_layout(pad=2.0)"
      ],
      "metadata": {
        "id": "AvMo8OS6t-ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll look at tuning the number of estimators, i.e. trees in the forest. So far we have been using the default value of 100. "
      ],
      "metadata": {
        "id": "6YG2Jo9f4VE7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhHVwO1dgpCb"
      },
      "outputs": [],
      "source": [
        "estimators_list= range(20, 200, 5)\n",
        "model_list = []\n",
        "train_score_list = []\n",
        "test_score_list = []\n",
        "\n",
        "for estim in estimators_list: \n",
        "    # split the training data and test data for each round\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df_features, rank, test_size= 0.2, random_state=42)\n",
        "    # initialize the random forest classifiers\n",
        "    clf = RandomForestClassifier(max_depth=10, n_estimators=estim);\n",
        "    clf.fit(X_train, y_train);\n",
        "    model_list.append(clf);\n",
        "    train_score = clf.score(X_train, y_train);\n",
        "    train_score_list.append(train_score);\n",
        "    test_score = clf.score(X_test, y_test);\n",
        "    test_score_list.append(test_score);\n",
        "    estim = format(estim, '.1f');\n",
        "    train_score = format(train_score, '.13f');\n",
        "    test_score = format(test_score, '.13f');\n",
        "    print(\"Random forest num estimators: {} || train_accuracy: {} || test_accuracy: {}\".format(estim, train_score, test_score));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gh2yRhjiqxk"
      },
      "outputs": [],
      "source": [
        "plt.plot(estimators_list, train_score_list, '-', label = 'train_accuracy')\n",
        "plt.plot(estimators_list, test_score_list, '--', label = 'test_accuracy')\n",
        "plt.title('{} Model Accuracy with different {} params'.format('Random Forest', 'n_estimators'))\n",
        "plt.xlabel('n_estimators')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc= 0, prop={'size': 10})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the number of estimators used didn't make a strong impact on the model accuracy so we will stick with the default of 100. \n",
        "\n",
        "We'll try one more time using PCA. "
      ],
      "metadata": {
        "id": "R2Eyba5k4pAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s8vLYScXNL7"
      },
      "outputs": [],
      "source": [
        "# Address scale-invariance\n",
        "sc = StandardScaler()\n",
        "x_scaled = sc.fit_transform(df_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i2dn-NsmoLE"
      },
      "outputs": [],
      "source": [
        "# Instantiate and Fit PCA\n",
        "pca = PCA(n_components=11)\n",
        "x_pca = pca.fit_transform(x_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceGasL4MmqtS"
      },
      "outputs": [],
      "source": [
        "pca_depth_list= range(2, 26, 2)\n",
        "pca_model_list = []\n",
        "pca_train_score_list = []\n",
        "pca_test_score_list = []\n",
        "\n",
        "for depth in pca_depth_list: \n",
        "    # split the training data and test data for each round\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x_pca, rank, test_size= 0.2, random_state=42)\n",
        "    # initialize the random forest classifiers\n",
        "    clf = RandomForestClassifier(max_depth= depth);\n",
        "    clf.fit(X_train, y_train);\n",
        "    model_list.append(clf);\n",
        "    train_score = clf.score(X_train, y_train);\n",
        "    train_score_list.append(train_score);\n",
        "    test_score = clf.score(X_test, y_test);\n",
        "    test_score_list.append(test_score);\n",
        "    depth = format(depth, '.1f');\n",
        "    train_score = format(train_score, '.13f');\n",
        "    test_score = format(test_score, '.13f');\n",
        "    print(\"Random forest tree max_depth: {} || train_accuracy: {} || test_accuracy: {}\".format(depth, train_score, test_score));"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, using PCA does not improve our accuracy. "
      ],
      "metadata": {
        "id": "hvX5dWz65LPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K Nearest Neighbors (KNN)"
      ],
      "metadata": {
        "id": "jFi1XHHuxjkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We decided to try some more models starting with K-Nearest Neighbours. The KNeighborsClassifier algorithm works by finding the K closest training examples in the feature space (i.e., the K nearest neighbors) for a given input sample and then making a prediction based on the majority class label of those K neighbors. "
      ],
      "metadata": {
        "id": "v2AE4sW36EJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "knn_classifier.fit(x_train, y_train)\n",
        "knn_predictions = knn_classifier.predict(x_test)"
      ],
      "metadata": {
        "id": "RcVlKR5_xlsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Find the accuracy and store the value in `test_accuracy`\n",
        "knn_accuracy = accuracy_score(knn_predictions,y_test)\n",
        "train_accuracy = knn_classifier.score(x_train, y_train)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Test accuracy: {knn_accuracy}\")\n",
        "# print the classification report\n",
        "print(classification_report(y_test, knn_predictions))"
      ],
      "metadata": {
        "id": "KTIOvYo5x0Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial accuracy of this model is lower than Random Forest but we decided to try tune the model further to see if there's an improvement we can make by changing the number of neighbours used. The default is 5."
      ],
      "metadata": {
        "id": "qshEdGDk7TQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_features, rank, test_size=0.2)\n",
        "\n",
        "# Define range of K values to test\n",
        "k_range = range(1, 11)\n",
        "\n",
        "# Use cross-validation to find the best K value\n",
        "val_scores = []\n",
        "for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=5)\n",
        "    val_scores.append(scores.mean())\n",
        "\n",
        "# Choose the best K value\n",
        "best_k = k_range[np.argmax(val_scores)]\n"
      ],
      "metadata": {
        "id": "9VkLILr97i7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(k_range, val_scores)\n",
        "plt.xlabel('Value of K for KNN')\n",
        "plt.ylabel('Cross-validated Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uz_cPEEp8QnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"best k value:\", best_k)"
      ],
      "metadata": {
        "id": "pB9J_opN8j7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the best k value is 1, this may overfit the training data set so we'll try run the model again with a k value of 3. "
      ],
      "metadata": {
        "id": "50YFP5uA81EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain the model using the best K value\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_features, rank, test_size=0.2)\n",
        "final_model = KNeighborsClassifier(n_neighbors=3)\n",
        "final_model.fit(X_train, y_train)\n",
        "knn_predictions = final_model.predict(X_test)\n",
        "\n",
        "#  Find the accuracy and store the value in `test_accuracy`\n",
        "best_accuracy = accuracy_score(knn_predictions,y_test)\n",
        "train_accuracy = knn_classifier.score(X_train, y_train)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Test accuracy: {best_accuracy}\")\n",
        "# print the classification report\n",
        "print(classification_report(y_test, knn_predictions))"
      ],
      "metadata": {
        "id": "4swJmJ6A8hzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The f1-score between classes is more balanced but the model is still not as good as the Random Forest model. "
      ],
      "metadata": {
        "id": "o16uhKK29kus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural networks"
      ],
      "metadata": {
        "id": "2MHhxKUmyX99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We decided to see if neural networks could perform better. We started with a simple feedforward Neural Network using the default single hidden layer of 100 nodes."
      ],
      "metadata": {
        "id": "WcB3_RZG9xTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Address scale-invariance\n",
        "sc = StandardScaler()\n",
        "x_scaled = sc.fit_transform(df_features)"
      ],
      "metadata": {
        "id": "lDCdt0J3-pnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, rank, test_size=0.2)\n",
        "snn_classifier = MLPClassifier()\n",
        "snn_classifier.fit(x_train, y_train)\n",
        "snn_predictions = snn_classifier.predict(x_test)"
      ],
      "metadata": {
        "id": "j6PClIZ5yXZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Find the accuracy and store the value in `test_accuracy`\n",
        "snn_accuracy = accuracy_score(snn_predictions,y_test)\n",
        "train_accuracy = snn_classifier.score(x_train, y_train)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Test accuracy: {snn_accuracy}\")\n",
        "# print the classification report\n",
        "print(classification_report(y_test, snn_predictions))"
      ],
      "metadata": {
        "id": "XSgh8349ykLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model performed well so we decided to use a deeper neural network with 5 hidden layers of 100 nodes each."
      ],
      "metadata": {
        "id": "RNzP8zx2-AwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, rank, test_size=0.2)\n",
        "dnn_classifier = MLPClassifier(hidden_layer_sizes = [100]*5)\n",
        "dnn_classifier.fit(x_train, y_train)\n",
        "dnn_predictions = dnn_classifier.predict(x_test)"
      ],
      "metadata": {
        "id": "PCFgnpaoy5nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Find the accuracy and store the value in `test_accuracy`\n",
        "dnn_accuracy = accuracy_score(dnn_predictions,y_test)\n",
        "train_accuracy = dnn_classifier.score(x_train, y_train)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Test accuracy: {dnn_accuracy}\")\n",
        "# print the classification report\n",
        "print(classification_report(y_test, dnn_predictions))"
      ],
      "metadata": {
        "id": "XC4rkm0uy7nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see evidence that the model has started to overfit the training data. "
      ],
      "metadata": {
        "id": "n3LRvPbJAnNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble of Binary Classifiers (One-vs-Rest)"
      ],
      "metadata": {
        "id": "aThsPcJpzX6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the one versus all method worked well using linear regression we decided to add on to the neural network model by training multiple binary classifiers. These are then combined to generate multi-class outputs. Since the simple neural network performed better than the model with 5 hidden layers we'll use that one here. "
      ],
      "metadata": {
        "id": "XZafBkPp_cr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, rank, test_size=0.2)\n",
        "dnns_classifier = OneVsRestClassifier(MLPClassifier())\n",
        "dnns_classifier.fit(np.array(x_train), y_train)\n",
        "dnns_predictions = dnns_classifier.predict(x_test)"
      ],
      "metadata": {
        "id": "5TEyDDVTzW0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Find the accuracy and store the value in `test_accuracy`\n",
        "dnns_accuracy = accuracy_score(dnns_predictions,y_test)\n",
        "train_accuracy = dnns_classifier.score(x_train, y_train)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Test accuracy: {dnns_accuracy}\")\n",
        "# print the classification report\n",
        "print(classification_report(y_test, dnns_predictions))"
      ],
      "metadata": {
        "id": "NKZK8NMQzobP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machines"
      ],
      "metadata": {
        "id": "Wb3X2Vuf0Azk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We decided to continue to experiment with different binary classifiers in this way. "
      ],
      "metadata": {
        "id": "CiYqvieUAcgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, rank, test_size=0.2)\n",
        "svm_classifier = SVC(decision_function_shape='ovr')\n",
        "svm_classifier.fit(x_train, y_train)\n",
        "svm_predictions_labels = svm_classifier.predict(x_test)"
      ],
      "metadata": {
        "id": "zQ1kQF4g0DON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Find the accuracy and store the value in `test_accuracy`\n",
        "svm_accuracy = accuracy_score(svm_predictions_labels,y_test)\n",
        "train_accuracy = svm_classifier.score(x_train, y_train)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Test accuracy: {svm_accuracy}\")\n",
        "# prithe classification report\n",
        "print(classification_report(y_test, svm_predictions_labels))"
      ],
      "metadata": {
        "id": "WTLQlJiD0RSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-Vs-Rest XGBoost Classifier"
      ],
      "metadata": {
        "id": "6reULQo6A3Bd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the Random Forest model performed well, we decided to try gradient boosting as it also uses decision trees. Boosting grows shallow trees sequentially to improve the trees that are already trained."
      ],
      "metadata": {
        "id": "f4dzXd9RBK3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, rank, test_size=0.2)\n",
        "xgb_classifier = OneVsRestClassifier(XGBClassifier())\n",
        "xgb_classifier.fit(np.array(x_train), y_train)\n",
        "xgb_predictions = xgb_classifier.predict(x_test)"
      ],
      "metadata": {
        "id": "SMtCPgM93eul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance\n",
        "\"\"\"print(xgb_classifier.feature_importances_)\n",
        "# plot\n",
        "pyplot.bar(range(len(xgb_classifier.feature_importances_)), xgb_classifier.feature_importances_)\n",
        "pyplot.show()\"\"\""
      ],
      "metadata": {
        "id": "e_uLE1xPBlYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Find the accuracy and store the value in `test_accuracy`\n",
        "xgb_accuracy = accuracy_score(xgb_predictions,y_test)\n",
        "train_accuracy = xgb_classifier.score(x_train, y_train)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Test accuracy: {xgb_accuracy}\")\n",
        "# prithe classification report\n",
        "print(classification_report(y_test,xgb_predictions))"
      ],
      "metadata": {
        "id": "VDc9ZpS736By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "rank = le.fit_transform(rank)\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_scaled, rank, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Create an XGBoost classifier object\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Create a GridSearchCV object to perform grid search\n",
        "grid_search = GridSearchCV(xgb, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV object on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the best validation score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print('Best hyperparameters:', best_params)\n",
        "print('Best validation score:', best_score)"
      ],
      "metadata": {
        "id": "bmWbG27-GTSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given this information, we could refit the model using these specific parameters and evaluate the performancy of the train and test set. "
      ],
      "metadata": {
        "id": "nCxTfOB1DJ11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Comparison and Conclusion\n",
        "\n",
        "* Random Forests, XGBoosting and the simple neural network showed the best performance. We'd have liked to have gotten a model with higher accuracy and one that was showing closer behaviour between test and train sets. The next steps we'd take are outlined below. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JV8VJitEVJFM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDW0V-6n2S4S"
      },
      "source": [
        "# Challenges and Obstacles Faced\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ9-Y45V2WvI"
      },
      "source": [
        "*  Lyricsgenius API couldn't be used from within colab so this had to be run locally.  \n",
        "*  It took over 24 hours to pull all the lyrics from Genius.com\n",
        "*  The data from Genius.com required investigation and cleaning because it contain junk from the web. \n",
        "* The size of our final dataset was smaller than we hoped due to the availability of lyrics and audio features. \n",
        "* The class imbalances were significant and ideally instead of imputing we would have collected more data. \n",
        "* It was difficult to run the whole notebook on the available RAM\n",
        "* We orginally tried to use more classes for prediction but found that all models had very low accuracy (under 0.4) with the data that we had\n",
        "* Working together on a Colab file can be tricky because the merge conflicts are difficult to resolve.\n",
        "* We are new to data science and there is a lot to learn so we would have loved some more time to tune the models even more and build up expertise. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjA-h-EZ2Xg3"
      },
      "source": [
        "# Potential Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHeN35Qw2XNF"
      },
      "source": [
        "We could try the following next:\n",
        "\n",
        "*  move our modelling to a distributed system like Apache Spark to increase the improve the performance. \n",
        "*  impute lyric values, i.e. sentiment, word counts to be able to include more rows of data in the dataset. A larger data set would be very useful so ideally we'd go back and get more data now that we've seen how the model performs with the volume of data that we currently have. \n",
        "*  continue to fine tune the paramenters that we used, perhaps using k-fold validation. \n",
        "* explore using different classes to see if we can get the models to predict more granular classes, e.g. number one hits. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}